#+TITLE: The Carth programming language

Features and other stuff to do/implement in/around Carth.

* TODO Migrate TODO to some kind of Kanban board or the like
Self hosted? Does Sourcehut have anything? Base it on mailing lists (no... or maybe?)

But I need something other than this file. It only pollutes the Git
history for no real good. It's also less useful now that I've had to
disable magit-todos.

Another reason is that this file doesn't scale at all. Not that I
expect or particularly desire to have contributors, but I also don't
want to scare anyone away xd.

* INACTIVE Package system
  We will need some kind of package-management solution, or we will
  end up like C where using dependencies is a chore.

  I kind of want to just use Guix somehow, but it should ideally be
  possible to use whatever package management solution on other
  distros as well...

  I'm thinking: the package manager shouldn't make too many
  assumptions about the system and get in the way in unorthodox
  scenarios. Consider how Guix and Nix have to use some hacks to get
  packages to play nice with the store, not make assumptions about the
  dir tree structure (like there being a /usr/lib, a /bin/bash) and
  stuff like that.
  
  See: https://drewdevault.com/2021/09/27/Let-distros-do-their-job.html
** Carth + Guix = üíú?
   Anv√§nd guix som pakethanterare + virtualenv f√∂r Carth, typ som Stack
   f√∂r Haskell och Cargo f√∂r rust.

   T√§nk att carth antingen drar in guix som bibliotek, eller p√• n√•got
   annat vis kr√§ver guix fr√•n systemet.

   T√§nk:
     carth init
   Skulle kunna k√∂ra
     guix environment
   och lite mer.

   Update <2021-10-20 ons>: I think pulling in guix as a dependency
   would not work well. That's not how it's designed to be used. Would
   also work weirdly when actually using Guix as the OS package
   manager on top as well. Also, asking users to install Guix and add
   our channel would not be ergonomic enough to be the simplest way to
   do things.

   Instead, I think we should do something like what this comment
   proposes: https://news.ycombinator.com/item?id=24847060, i.e.,
   imitate cargo. Cargo is fairly primitive, and maybe allows "too
   much" when it comes to using different versions, but it's fairly
   simple and does its job well. Then we can "bless" Guix as a package
   manager to use on top of our primitive cargo-clone for more
   advanced features, like releases with compatible packages,
   environments, etc etc. Basically, we don't put too much
   responsibility on our own shoulders, and let OS distributions do
   what they do best. Make sure that cargo-clone never gets in the way
   of Guix and other distributions. For example, overriding dependency
   config to use local files and other versions should have first
   class support.

   Maybe help maintain the Guix (& Nix?) packages myself.
   
*** Extra s√§kert?
    "guix environment allows you to run any application in an isolated
     container that restricts network access, hides the filesystem (no
     risk that the closed-source program steals any of your files, say
     your Bitcoin wallet or your PGP keys) and even system-level
     information such as your user name. This is essential to run any
     non-trusted, closed-source program."

    Skulle kanske f√∂rhindra vissa av problemen man sett i NPM, typ "event-stream" debaklet.

*** L√§s mer
    https://www.gnu.org/software/guix/manual/en/guix.html#Channels
  
* NEXT Module system
  Postfix syntax for module paths? A bit like web-domains -
  "sub.main.top". E.g. "vector.collections.std".  Most relevant
  information floats to the left. Maybe a good idea, maybe
  not. Consider it.

  Look at ML modules.

** INACTIVE Allow conflicting imports if unambiguous?
   I'm thinking something that would allow the following. It would be
   less annoying than having to qualify everything. Also, gotta think
   about how this relates to overloading √† la C++.

   #+BEGIN_SRC carth
   (module Foo
           (data FooThing First Second)
           (define: isFirst
               (Fun FooThing Bool)
             (fun-match
               [First True]
               [Second False])))

   (module Bar
           (data BarThing First Second)
           (define: isFirst
               (Fun BarThing Bool)
             (fun-match
               [First True]
               [Second False])))

   ;; First, there should be no error for just importing modules with conflicting
   ;; defs. This is ok in Haskell, unless one of the conflicting defs is used.
   (import Foo)
   (import Bar)

   ;; Second, it should be allowed to use one of a set of conflicting defs if the
   ;; type makes it unambiguous....

   ;; either explicitly
   (define: x FooThing First)
   (define: y BarThing First)

   ;; or implicitly
   (define t (isFirst x))
   (define u (isFirst y))
   #+END_SRC

* NEXT Bidirectional type checking
I'm not fully convinced yet, but I believe we might want to use
bidirectional type checking instead of a unification based, HM-like
typechecker in Carth.

HM shares a few properties with bidirectional typechecking, like
implicit type abstraction / application, but it's not the same
thing. Proper bidirectional typechecking would give us an easy way to
do implicit numeric coercions for proper subtypes, afaik.

- https://lobste.rs/s/mhdvzh/appeal_bidirectional_type_checking
- https://www.haskellforall.com/2022/06/the-appeal-of-bidirectional-type.html
- Complete and Easy Bidirectional Typechecking for Higher-Rank Polymorphism, by Jana Dunfield & Neelakantan R. Krishnaswami
- Bidirectional Typing, by JANA DUNFIELD & NEEL KRISHNASWAMI
* NEXT Typeclasses
  Need some kind of system like type classes for ad hoc
  polymorphism. Maybe Haskell style type classes, Agda style
  implicits, or Ocaml style modules. Not sure.

  "Type classes are functions from types to expressions"
  https://youtu.be/5QQdI3P7MdY?t=920. Interesting thought! Can we view
  type families the same way, but functions from types to types or
  smth? Maybe we can come up with more intuitive terminology.

  https://www.microsoft.com/en-us/research/wp-content/uploads/1994/04/classhask.pdf
  https://static.aminer.org/pdf/PDF/000/542/781/implementing_type_classes.pdf

** Agda style classes w implicit args
   https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#implicit-parameters

   In Haskell, you can only have a single instance of a specific
   typeclass for a specific type. This doesn't always make
   sense. Consider Semigroup for Int. Both + and * make sense, but we
   can only have one unless we goof around with newtypes etc, and that
   kinda sucks.

   Consider an approach more like agda. That model is more lika basic
   Hindley-Milner + dictionsry passing, except the "typeclass"
   argument can be passed implicitly with the {} syntax! That seems
   really cool.

   I'm not sure how implicit arguments work though. Does the compiler
   just look at all available bindings and pick the first/only
   available variable of that type?

   https://agda.readthedocs.io/en/v2.5.2/language/implicit-arguments.html

   https://agda.readthedocs.io/en/v2.5.2/language/instance-arguments.html

   Or just do it kind of Haskell style, but give the instances names
   and allow multiple, overlapping instances, raisi g an error if the
   instance is ambiguous somehow.

   Problem with instances as implicit arguments:
   https://youtu.be/2EdQFCP5mZ8?t=1259.  We'd have to know exactly
   which instances exist for the same type, and from where they're
   imported and what scoping they'll have. That sucks. Another
   horrible thing: imagine creating a sorted list with one instance, and doing
   a sorted lookup with another (accidentally or not), you could an incorrect
   result with no error from the compiler!

   Maybe an alternative could be to have both ~primary~ and
   ~secondary~ instances, where the primary instances may not overlap
   or be orphaned, like Rust, but may be passed implicitly, while
   secondary instances may overlap and be orphaned, but must be
   "overriden"/passed explicitly.

   But that may also not work. For the following code,

   #+BEGIN_SRC haskell
   foo :: Foo a => a -> a
   foo = bar

   bar :: Foo a => a -> a
   bar = ...
   #+END_SRC

   consider that we call ~foo~ with an explicit secondary
   instance. What instance will ~bar~ be given? If we must pass
   secondary instances explicitly, it seems ~bar~ would get the
   primary instance, and ~foo~ and ~bar~ would be called with
   different instances. BAD!

   Probably last update for this section: [[https://old.reddit.com/r/haskell/comments/765ogm/multiple_type_class_instances_for_the_same_type/][this thread]] has convinced me
   that Haskell-/Rust-style typeclasses is the best idea.

* NEXT Linear types
  Linear types would allow predictable performance and behaviour of
  e.g. IO tasks. Force a single manual file-close or
  buffer-flush. Force a single free for malloc.  Affine types would
  allow better performance.  E.g. pure, in-place modification of
  array.  If noone else points to it, value can be consumed and
  modified rather than cloned. Something like: ~fn push(mut v:
  Vec<i32>, x: i32) -> Vec<i32> { v.push(x); v }~ Implemented as maybe
  a wrapper, or an interface?  Maybe like in haskell with lolly
  operator?

  Things to consider: Linear arrow vs. `kind` approach or similar?

  Check out Idris Uniqueness types, Linear Haskell's linear arrows,
  and however Blodwen does it (linear arrows kind of I think).

* NEXT Higher kinded types

* INACTIVE Type families / functional dependencies and multi-param classes / Dependent types
  I'm on the fence here, but the consensus seems to be that type
  families are better than fundeps. Also, it might be possible to
  avoid needing to implement Multi-parameter typeclasses if type
  families are available to compensate. Seems that would reduce
  ambiguities and mental overhead a bit.

  Neither type families or fundeps are necessary if we have dependent
  types, but that would likely bring difficulties of it's own.

  Type families in Haskell vs Dependent types in a pseudo-Haskell vs
  Dependent types in Agda:

** Sketch
   The wiki page is
   good. https://en.wikipedia.org/wiki/Type_family. Haskell wiki also
   has some interesting notes
   https://wiki.haskell.org/GHC/Type_families.

   https://en.wikipedia.org/wiki/Lambda_cube

   Does it complicate typechecking? It's not obvious to me how it
   would?

   In haskell, type families and data families are always
   open. Probably fine to keep it that way? Not sure the complexity of
   having both open and closed versions are worth it?

   Relations:
   - Function :: Value -> Value
   - Typeclass :: Type -> Values
   - Typefamily :: Type -> Type
   - Dependent type :: Value -> Type

   I don't love the names "family" and "class". Could we use something
   that makes more clear the relations above? Like "type function" or
   something? Although, I guess at least "class" wouldn't be so bad to
   keep, for familiarity reasons.

   Do we need data families as well? I'd prefer not to have to add
   them also. A little bit of inconvenience remaining is worth it if
   we can avoid a lot of complexity in the language.

   Observation: Type families are just type aliases, but we can
   pattern match on the input.

   Observation: A typeclass with associated types is basically an
   extension of normal typeclasses that makes it (Type -> (Type,
   Value)). Defining an associated type in an instance of a typeclass
   is basically a way of allowing one to add cases to the pattern
   matching after definition. Consider this:

   #+BEGIN_SRC carth
   (type (Foo a)
     (Match a
            (case Bar Int)
            (case Baz Bool)))
   #+END_SRC

   this is the same as

   #+BEGIN_SRC carth
   (class (Foo' a)
     (type (Foo a)))

   (instance (Foo' Bar)
     (type (Foo Bar) Int))

   (instance (Foo' Baz)
     (type (Foo Baz) Bool))
   #+END_SRC

   The difference being that with the typeclass version of
   typefamilies, cases/definitions can be separated from the
   declaration, and user modules can extend the type family by adding
   another instance.

   #+BEGIN_SRC carth
   ;; Warning: some pseudocode and unimplemented features

   ;; The different possible forms, which would be basically
   ;; equivalent. Each could be convenient, but not sure if
   ;; it's a good idea to implement all.

   ;; Single case

   ;; Alias form
   (type (Option a) (Maybe a))

   ;; <=> closed case form
   (type (Option a)
     (case (_) (Maybe a)))

   ;; <=> open case form
   (type (Option a))
   (type case (Option _) (Maybe a))

   ;; <=> class form
   (class (Foo a)
     (type Option))
   (class case (Foo a)
          (type Option (Maybe a)))


   ;; Multiple cases

   ;; Can't be described as alias
   ...

   ;; closed case form
   (type (Result ok err)
     (case (_ Unit) (Maybe ok))
     (case (_ _)    (Either err ok)))

   ;; <=> open case form
   ;;
   ;; Unlike value pattern matching, order shouldn't matter, as
   ;; we could be defining each case in a different
   ;; package. Some other algorithm for handling overlapping
   ;; instances would have to be used.
   (type (Result ok err))
   (type case (Result ok err)  (Either err ok))
   (type case (Result ok Unit) (Maybe ok))

   ;; <=> class form
   (class (Foo ok err)
     (type Result))
   (class case (Foo ok err)
          (type Result (Either err ok)))
   (class case (Foo ok Unit)
          (type Result (Maybe ok)))
   #+END_SRC

   Typeclass (Type, Values) vs Type family + normal typeclass:

   #+BEGIN_SRC carth
   ;; 1

   ;; should implicitly create namespace `Iter`, so it's `Iter/Item` and `Iter/next`
   (class (Iter it)
     (type Item)
     (: next (Fun it (Maybe [Item it]))))

   (class case (Iter (Array a))
          (type Item a)
          (define (next arr) ...))

   ;; 2
   ;; <=> (except for namespacing)

   (type (Iter-item it))
   (type case (Iter-item (Array a)) a)

   (class (Iter it)
     (: next (Fun it (Maybe [(Iter-item it) it]))))

   (class case (Iter (Array a))
          (define (next arr) ...))
   #+END_SRC

   And in real Haskell that compiles, for comparison:

   #+BEGIN_SRC haskell
   -- 1

   class Iter i where
       type Item i
       next :: i -> Maybe (Item i, i)

   instance Iter [a] where
       type Item [a] = a
       next = \case
           [] -> Nothing
           a : as -> Just (a, as)

   -- 2

   type family Item' i
   class Iter' i where
       next' :: i -> Maybe (Item' i, i)

   type instance Item' [a] = a
   instance Iter' [a] where
       next' = \case
           [] -> Nothing
           a : as -> Just (a, as)
   #+END_SRC

   https://blog.rust-lang.org/2021/02/11/Rust-1.50.0.html#a-niche-for-file-on-unix-platforms

** Type families, Haskell
   #+BEGIN_SRC haskell
   class Iter c where
       type Item c
       next :: c -> Maybe (Item c, c)

   nextList :: [a] -> Maybe (a, [a])
   nextList = \case
       [] -> Nothing
       a : as -> Just (a, as)

   instance Iter [a] where
       type Item [a] = a
       next = nextList
   #+END_SRC

** Dependent types, pseudo-Haskell
   #+BEGIN_SRC haskell
   class Iter c where
       item :: Type
       next :: c -> Maybe (item, c)

   nextList :: [a] -> Maybe (a, [a])
   nextList = \case
       [] -> Nothing
       a : as -> Just (a, as)

   instance Iter [a] where
       item = a
       next = nextList
   #+END_SRC

** Dependent types, Agda
   #+BEGIN_SRC agda2
   record Iter (C : Set) : Set1 where
     field
       item : Set
       next : C -> Maybe (item √ó C)

   nextList : {A : Set} -> List A -> Maybe (A √ó List A)
   nextList [] = nothing
   nextList (x ‚à∑ xs) = just (x , xs)

   listIter : {A : Set} -> Iter (List A)
   listIter {a} = record
     { item = a
     ; next = nextList
     }
   #+END_SRC

* NEXT RC / ARC / Refcount / reference counting
GC is inelegant, needing to stop the world or use a bunch of complex
methods. Also, latency is bad.

Do good refcounting instead.

There were other reasons, but I can't remember them of the top of my
head.

https://atp.fm/205-chris-lattner-interview-transcript#gc /
https://news.ycombinator.com/item?id=31139610

#+BEGIN_QUOTE
Chris Lattner: Here‚Äôs the way I look at it, and as you said, the ship
has somewhat sailed: I am totally convinced that ARC is the right way
to go upfront. It is better in a whole bunch of different ways. It
gives you deterministic behavior, so it doesn‚Äôt have the
unpredictable-stutter problem that people like to bash on GCs.

The stutter problem, to me, isn‚Äôt really the issue, even though
[1:59:30] that‚Äôs what GC-haters will bring up all the time. It‚Äôs more
about being able to reason about when the memory goes away. The most
important aspect of that is that ARC gets rid of finalizers.

If you use a garbage-collected language, you use
finalizers. Finalizers are the thing that gets run when you‚Äôre not
object gets destroyed. Finalizers have so many problems that there are
entire bodies of work talking about how to work around problems with
finalizers.

For example: the finalizer gets run on the wrong thread, it has to get
run multiple [2:00:00] times, the object can get resurrected while the
finalizer‚Äôs running. It happens non-deterministically later. You can‚Äôt
count on it, and so you can‚Äôt use it for resource management for
database handles and things like that, for example. There are so many
problems with finalizers that ARC just defines away by having
deterministic destruction.

There are two arguments that people make [2:00:30] against ARC in
favor of a tracing garbage collector, one of which is that ARC adds
overhead because you have retain and release operations that run. That
is true. The other is that you have to think about cycles in ARC
because it doesn‚Äôt automatically collect cycles, and that is also
true.

The rebuttal I‚Äôd give to people is that those problems are also true
in garbage collection, just in different ways. In a garbage collector,
for example, [2:01:00] people don‚Äôt think about it, but garbage
collection injects additional code into your application just like ARC
does.

There are many different garbage collection algorithms, and not all of
them are the same. But most modern garbage collectors, that use a
nursery for short-lifetime objects then promote them out ‚Äî that are
generational ‚Äî use something called a write barrier. Every time you
store to a property [2:01:30] of an object, say, you have to run
additional code.

Garbage collectors also need the ability to stop all the threads, or
at least to be able to stop threads at some point in time, and they
need to be able to do so within a specific time bound because they
don‚Äôt want the garbage collector to take forever. The artifact of that
is that typical garbage collectors, in Java for example, will
introduce what‚Äôs called a safepoint into loops. So now, in your loops,
extra code is being run because of the garbage collector.

On more [2:02:00] aggressive garbage collection algorithms ‚Äî for
example, I was reading a blog post recently about Go‚Äôs tricolor
algorithm ‚Äî they‚Äôre touting the advantage of really low latency and
the ability to guarantee response times in a more fine-grained level
than most garbage collectors. But to do that, they use this tricolor
algorithm which dramatically lowers throughput, because they‚Äôre doing
almost exactly the same kinds of operations that ARC is doing.

The problem [2:02:30] that it then introduces, though, is that these
operations that the garbage collector is introducing are sometimes but
not nearly as well optimizable as the ARC overhead that the ARC
optimizer applies to.

Furthermore, there‚Äôs no out on it. With ARC, I think and hope that the
ownership model will give people the ability to take control of those
overheads. And if it becomes a problem in practice, or if they‚Äôre just
that kind of person, they can take full control over the lifetime of
their objects, and then know that ARC will never happen. In a garbage
collector, you don‚Äôt have that.

[2:03:00] The performance side of things I think is still up in the
air because ARC certainly does introduce overhead. Some of that‚Äôs
unavoidable, at least without lots of annotations in your code, but
also I think that ARC is not done yet. A ton of energy‚Äôs been poured
into research for garbage collection, particularly since Java has come
up. There‚Äôs been hundreds of papers written in the academic circles,
tons of work in HotSpot and other Java [2:03:30] implementations to do
different tweaks and different tunings and different new kinds of
algorithms in garbage collecting. That work really hasn‚Äôt been done
for ARC yet, so really, I think there‚Äôs still a a big future ahead.

On the programming side of things, the cycle side of things, I think
it‚Äôs also a really interesting question of how much should people
think about memory?

When I was baiting you a little bit, you said that the great thing
about garbage collection is that you don‚Äôt have to think about
memory. Of course we know that‚Äôs not true, right? Because if [2:04:00]
you have a reference to some big object graph that you didn‚Äôt mean to
keep around (maybe it‚Äôs in your undo stack), then you will ‚Äúleak‚Äù that
memory. That‚Äôs true of a garbage collector, and that‚Äôs true of ARC as
well. Any automatic memory-management approach has that problem.

There‚Äôs this question of if you‚Äôre building a large scale system, do
you want people to [2:04:30] ‚Äúnever think about memory?‚Äù Do you want
them to think about memory all the time, like they did in
Objective-C‚Äôs classic manual retain-and-release? Or do you want
something in the middle?

I think that ARC strikes a really interesting balance, whether it‚Äôs in
Objective-C or Swift. I look at manual retain-and-release as being a
very imperative style of memory management, or malloc and free, where
you‚Äôre telling the code, line by line: this is where you should do a
reference-count operation, [2:05:00] this is where you should release
the memory, this is what you should do at this point in time.

ARC then takes that model and bubbles it up a big step, and it makes
it be a very declarative model. So instead of telling the compiler
that this is the place that you should do a retain, you instead say,
‚ÄúThis is an owning relationship.‚Äù The cool thing about that to me is
that not only does it get rid of the mechanics of maintaining
reference counting and define away tons of bugs by doing that, it also
means that [2:05:30] it is now explicit in your code what your
intention was. That‚Äôs something that people who maintain your code
benefit from.

By saying that I have a weak point or two, the parent object of my
thing, that‚Äôs a really important relationship to know about and as
you‚Äôre looking at the code, you‚Äôre maintaining the code. Having that
be explicit is very valuable, because that talks about the
relationship between values. To me, again with the goal of being able
to write large scale applications in Swift, I think that‚Äôs really
useful. [2:06:00] I also don‚Äôt think it‚Äôs hugely burdensome, though
it‚Äôs definitely part of the learning curve of learning how Swift works
that it has to be balanced in there as well.

So I don‚Äôt know. ARC has clear advantages in terms of allowing Swift
to scale down to systems that can‚Äôt tolerate having a garbage
collector, for example, if you want to write firmware in Swift. I
think that it does provide a better programming model where
programmers think just [2:06:30] a little bit about memory. And I
think that going forward, it provides a really high performance model
that you can get better than garbage collection in almost every way. I
think that in terms of trade-offs, it‚Äôs the right one to push forward.

The third piece that garbage collection is really bad about, which is
kind of a showstopper for Swift, is interoperability with C code. If
you‚Äôve ever worked with Java or other [2:07:00] similar
garbage-collected languages, one of the major advantages the garbage
collectors give you is that they move objects, and they need to do
that so they can compact those objects so they can then efficiently do
allocations. The problem is that once you start moving objects around,
if you‚Äôre interfacing with C code, you can‚Äôt have some random C code
having a pointer to your object and have it move because then you get
a dangling pointer.

Once you get down that line, you end up with things like JNI, the Java
Native Interface, where you have to [2:07:30] explicitly pin things,
you have to maintain them, it‚Äôs very complicated, it‚Äôs really
buggy. ARC completely defines this away by just saying that
something‚Äôs in memory, It has predictable lifetime, you can reason
about it. Swift provides tools for dealing with unsafe pointers and
things like that, and that makes the interoperability with existing C
code ‚Äî but also with Objective-C, and maybe someday C++ code ‚Äî really
simple, really natural and really efficient. I think that‚Äôs a huge
advantage that ARC [2:08:00] provides that really would be impossible
to do with a garbage collector.

That‚Äôs my opinion. I think reasonable people disagree, obviously, but
it‚Äôs something that does come up now and then.
#+END_QUOTE

https://gankra.github.io/blah/deinitialize-me-maybe/

* CANCELLED Custom GC
  Update <2022-05-24 tis>: I've actually changed my mind about
  refcounting. With some ownership analysys, which we'd need anyways
  for linear types, one could easily ommit most RC increments /
  decrements in the generated code. And predictable deinitialization +
  no GC latency is actually really valuable.

  Until we get linear types, and even then, we'll need some form of
  GC. Boehm's seems to be working well enough, but a conservative
  collector is not ideal, and I think it would be a fun project to
  write my own GC.

  There are many problems with refcounting: Generated llvm ir/asm gets
  polluted; While performance is more predictable, it's typically
  worse overall; Cycle breaking would either require using weak refs
  where appropriate, which would in turn require user input or an
  advanced implementation, or a periodic cycle breaker, which would be
  costly performance wise. So tracing GC is probably a good idea.

  GHC seems to prefer throughput over latency, so very long pauses are
  possible when you're working with a nontrial amount of data. "You're
  actually doing pretty well to have a 51ms pause time with over 200Mb
  of live data.".

  It could be interesting to add ways of controlling when GC happens
  so you can reduce spikes of latency. Haskell has ~performGC :: IO
  ()~ that does this. [[https://old.reddit.com/r/haskell/comments/6d891n/has_anyone_noticed_gc_pause_lag_in_haskell/di0vqb0/][Here is a gameboy]] who eliminates spikes at the
  cost of overall performance by calling ~performGC~ every frame.

  [[https://github.com/rust-lang/rfcs/blob/master/text/1598-generic_associated_types.md][Some inspiration here]].

  A tracing GC would be quite separate from the rest of the
  program. The only pollution would be calls to the allocator (not
  much different from the current sitch w malloc) and
  (de)registrations of local variables in Let forms (a total of two
  function calls per heap allocated variable).

  Implementing a tracing GC would be a fun challenge, and I'm sure it
  could be fun to try different algorithms etc.

  Look at
  - https://github.com/mkirchner/gc
  - https://youtu.be/FeLHo6tIgKI

* INACTIVE Effect system
  tags: Algebraic effects
  
  Seems like it could be more elegant than monad transformers,
  although maybe not as fast?

  Effect fusion seems to make it faster?

  Read Wu, Schrijvers 2014, 2015, 2016. I think their papers basically
  present the concept of fused effects.

  github.com/fused-effects/fused-effects

  https://youtu.be/vfDazZfxlNs?t=1730

  ^ det makear sense. Bygg basically upp ett tr√§d av den h√§r datatype,
  och interpreta det med alla handlers. Varje handler kollar om det √§r
  dens variant, och isf k√∂r effekten. F√∂r varje handler blir tr√§det
  simplare, och till sist √§r det bara Pure kvar.

  Naiv implementering ineffektiv. Bara t√§nk -- m√•ste interpreta ett
  tr√§d ist f√∂r att bara *g√∂ra* effekterna direkt!

  Man kan anv√§nda free monads f√∂r att bygga upp tr√§det, men detta √§r
  inte s√• effektivt.

  Grundid√©n med papret "fusion for free" √§r att man vill bara traversa
  tr√§det en g√•ng, och inte en g√•ng per effect handler.

  Med "fusion" verkar de syfta p√• funktionaliteten i GHC, att man kan
  fusionera ihop funktionsanrop av specifika m√∂nster till mer
  effektiva varianter. E.g., ~map f . map g~ fusioneras till ~map (f
  . g)~. P√• liknande vis fusioneras ~fold handleState . build . fold
  handleReader~ till bara ~fold (handleState . handleReader)~. Kan vi
  l√∂sa detta utan kompilatorst√∂d, eller √§r det kanske v√§rt att l√§gga
  till?

  See the talk on polysemy, it's a good complement and alternative to
  the fused effects one. https://youtu.be/-dHFOjcK6pA.

  We need type-level lists or sets, and a way to implement Member on
  that thing. If tuple types could contain higher kinded types, I
  think we only need classes.

  See:
  - https://youtu.be/z8SI7WBtlcA, https://youtu.be/z8SI7WBtlcA?t=1433
  - Eff language
  - https://youtu.be/XAnFUwIaZB8

** INACTIVE Memory allocation as an explicit effect
   In Rust, you can override the global memory allocator. Situational
   override is not really possible? I think either you use the global
   allocator, or you allocate with e.g. an arena explicitly.

   In Zig, all allocation is explicit, and you have to pass around
   whichever allocator you want the functions to use. Pro: easy to
   override allocation for an object or sub-program with e.g. an
   arena. Con: verbose, bothersome, less convenient.

   Maybe we could make heap allocations sort of semi-explicit in
   Carth, via an Effect system? Easy to override with e.g. arena
   allocator for specific functions, and not as inconvenient as
   Zig. Do-notation (or better? (like generalized application)) could
   make it fairly convenient, and there really is some usefulness to
   doing it. Would encourage keeping things on the stack whenever
   possible. But maybe it's too much inconvenience for a high-level
   lang? I mean, couldn't pretty much any closure actually heap
   allocate for the captures? Hmm.
  
* INACTIVE Property system
  I'm thinking of a system where you annotate functions in a source
  file with pre- and postconditions, which can then be checked in
  different modes depending on how much time you've got etc.

  - Proof-mode. Exchaustive checking of conditions. All possible
     inputs are generated, and the system checks that the precondition
     always implies the postcondition.
  - Test-mode. Statistical, random testing. Generate enough inputs
    such that the precondition is fulfilled for a statistically
    significant subset of the complete set of possible inputs.
  - Debug-mode. Functions are not tested ahead of time, instead
     assertions are inserted and checked at runtime.
  - Release-mode. Conditions are completely ignored.

* INACTIVE Hoogle equivalent
  https://wiki.haskell.org/Hoogle

* INACTIVE Playground
  Like play.rustlang.org

  https://play.rust-lang.org/help
  https://github.com/google/nsjail

  Might actually be pretty easy by making use of Guix
  containers. Sandboxes the filesystem, and doesn't give network
  access unless `--network` is provided.

  #+BEGIN_EXAMPLE
  guix environment --container --ad-hoc coreutils clang carth
  #+END_EXAMPLE
* INACTIVE Language server protocol
  [[https://github.com/Microsoft/language-server-protocol]]
  [[https://internals.rust-lang.org/t/introducing-rust-language-server-source-release/4209]]

* INACTIVE HTML documentation generation
  Like [[https://www.haskell.org/haddock/][haddock]] and [[https://www.haskell.org/haddock/][rustdoc]].

* INACTIVE Documentation checker
  Like a typechecker-pass but for generated documentation. Verify that
  all links are alive, that examples compile and produce the expected
  output, etc.
* Standard library (std, stdlib)
  Prefer somewhat big / wide stdlib. Small / bad standard library +
  good package manager => npm / cargo situation, where everything has
  sooo many dependencies. Having a dep is not bad per say, but when
  the numbers completely blow up, like in rust- and javascript-land,
  things can get messy. The best way to avoid this, I think, is having
  a standard library that has you covered for most common things.

  Examples of libraries in other ecosystems that should be part of the
  stdlib: `is-even` in JavaScript, `composition` in Haskell, `rand` in
  Rust.

  Go seems to have done this relatively well. Their stdlib has
  everything from JPEG codec, to a webserver. The stdlib shouldn't
  have everything though, as that will add a bunch of legacy cruft
  over time, like in Java. Would not be as much of a problem if we're
  not afraid of releasing new major versions removing deprecated
  stuff.

  Maybe separate stdlib into core and std. Core could be a smaller
  subset which is pretty much purely implemented in carth, so it's
  easy to use with interpreter and comptime. Conditional compilation
  to use efficient C/Rust versions normally.

** INACTIVE Numbers, algebra, mathematics
   How to best structure the numeric typeclasses? ~Num~ in Haskell is
   a bit coarse. For example, you have to provide ~*~, which doesn't
   make much sense for ~Vec3~, so you can't give a proper instance for
   ~Vec3~ to get ~+~. Maybe [[https://hackage.haskell.org/package/numeric-prelude-0.4.3.3][numeric-prelude]] could be a good
   alternative to look at?

   [[https://typeclasses.com/featured/to-integral-sized][toIntegralSized]]
*** INACTIVE Division of integers should return Rational?
    Lossless etc. No truncation by accident. SBCL LISP does this I think?

    Consider type size and overflow though. Maybe only do this for
    arbitrary-sized Integer, and not for fixed-sized Int.
** INACTIVE Concurrency / parallelism primitives
   Mutex, semaphore, etc.

   Look at how Rust and Haskell do it.

   Also, look at the crate [[https://crates.io/crates/parking_lot][parking_lot]], which does replaces the
   standard Rust primitives with smarter ones. E.g. the mutex does a
   small number of spins first, to avoid expensive thread juggling by
   the OS when the critical section is very short, but resort to the
   usual process interrupts in case it goes on for longer, to avoid
   priority inversion which is a problem with spinlocks.
   https://matklad.github.io/2020/01/02/spinlocks-considered-harmful.html
   https://matklad.github.io/2020/01/04/mutexes-are-faster-than-spinlocks.html

   Lock Free Data Structures using STM in Haskell: https://www.microsoft.com/en-us/research/wp-content/uploads/2006/04/2006-flops.pdf

** INACTIVE Random number generation
   References:
   - [[https://arxiv.org/abs/1910.06437][It is high time we let go of the Mersenne Twister]]
** NEXT Some algorithms & data structures
  We need good collections & algs for sorting etc. if Carth is going
  to be of any use to anyone. Would also be a good way to add to the
  set of test-programs & find the worst pain points of current Carth.

  Many of these have implementations to look at and compare to on
  [[rosettacode.org]].

  This list is sort of off the top of my head, so some might not be
  good fits in a purely functional language. Look at some resource on
  persistend data structures as well.

  - Priority queue
  - Binary tree (2-3 tree better?)
  - B-tree (specifically 2-3 tree?)
  - Random number generator
  - bubble, insertion, selection sort
  - quicksort
* INACTIVE "Global" memoization
  This is just an idea I had, and may or may not be wise to implement.

  Add a special function for "memoized application" that acts like the
  application function (in Haskell, ($) :: (a -> b) -> a -> b), the
  difference being that it stores the result in a global, hidden Map
  from function pointers and arguments to results. The user can then
  selectively memoize certain functions (or even just certain
  applications of the function), and not others -- the wise choice
  would be to not memoize cheap functions, but do memoize computation
  heavy functions. This is perfectly legal if the language is
  completely pure, as there can be no side-effects that are not
  repeated properly yada yada.

  An alternative could be that the user can mark a function definition
  as memoized, and then it's always memoized, not just certain
  applications. Also, there could then be a unique Map for each such
  function.
* INACTIVE Async I/O
  Zig seems to have a smart solution that doesn't require a separate
  `async` version of the standard library, unlike Rust with
  `async-std`.

  https://ziglang.org/download/0.6.0/release-notes.html#Async-IO

  Also look at how Haskell does it. It's probably smart.

* INACTIVE Boxing to allow for dynamic linking
  Boxing vs monomorphization. Boxing results in smaller binary and
  dynamically-linkable interface, but results in slower code (but not
  necessarily always, and maybe not by much!).

  Read /Tristan Hume - A Tour of Metaprogramming Models for Generics/
  for an overview of how different languages implement
  generics. [[https://thume.ca/2019/07/14/a-tour-of-metaprogramming-models-for-generics/][online]], [[file:~/Syncthing/books/papers/Tristan Hume - A Tour of Metaprogramming Models for Generics.html][locally]].

  When compiling a library, especially a dynamically linked one, how
  do we allow the export of polymorphic functions? We can't really use
  monomorphization, as we can't predict which types there should be
  instantiations for. Boxing would solve this problem and result in a
  smaller binary, but the code would most likely be slower, and the
  FFI would become more complicated.

  Maybe monomorphize all package-internal code, and require boxing for
  all public-facing polymorphic functions? Could require some keyword
  or special form, like `boxed`, to make it clear when the FFI will be
  affected.

  <2021-06-21 m√•n>: Try implementing polymorphism w boxing (& dict
  passing). Mono may really not be all that great, and it's really not
  that elegant. Big code size, slow compile times, no HRT, etc. Look
  at my own old post.

  https://www.reddit.com/r/ProgrammingLanguages/comments/npn3cd/what_are_some_anti_features_in_a_language/

  "With that said, I agree that eager monomorphization is an error, in my book.

   In a sense, monomorphization is exactly like inlining
   (copy/pasting). It feels strange that compilers would have complex
   heuristics to determine when to inline, when not to, and even in
   recent releases when to outline and yet... they just monomorphize
   everything template/generic without pause."

  Maybe box by default, and box all external functions, but like
  inlining, do monomorphization of appropriate function instantiaitons
  heuristically.

  From Tristan's text, on Haskell's dictionary passing:

  "Another way of implementing dynamic interfaces than associating
   vtables with objects is to pass a table of the required function
   pointers along to generic functions that need them. This approach
   is in a way similar to constructing Go-style interface objects at
   the call site, just that the table is passed as a hidden argument
   instead of packaged into a bundle as one of the existing arguments.

   This approach is used by Haskell type classes although GHC has the
   ability to do a kind of monomorphization as an optimization through
   inlining and specialization."

  See [[https://www.youtube.com/watch?v=ctS8FzqcRug][Switf's approach with the Value Witness Table]]. Basically,
  instead of passing generic types as completely opaque boxes, pass
  them as more of a sort of trait object, with some bundles functions
  for allocating and copying the type on the stack etc. Otherwise we
  have to store everything on the heap, even primitive types?

  Above paragraph is slightly misleading. Tristan explains witness
  tables well:

  "Swift makes the interesting realization that by using dictionary
   passing and also putting the size of types and how to move, copy
   and free them into the tables, they can provide all the information
   required to work with any type in a uniform way without boxing
   them. This way Swift can implement generics without
   monomorphization and without allocating everything into a uniform
   representation!  They still pay the cost of all the dynamic lookups
   that all boxing-family implementations pay, but they save on the
   allocation, memory and cache-incoherency costs. The Swift compiler
   also has the ability to specialize (monomorphize) and inline
   generics within a module and across modules with functions
   annotated @inlinable to avoid these costs if it wants to,
   presumably using heuristics about how much it would bloat the code.

   This functionality also explains how Swift can implement ABI
   stability in a way that allows adding and rearranging fields in
   structs, although they provide a @frozen attribute to opt out of
   dynamic lookups for performance reasons."

  This sounds really good! Single definition generation without
  expensive boxing! Monomorphization as an optimization!

  Value Witness Table in Swift seems to contain:
  
  - Size
  - Alignment
  - Copy constructor
  - Move constructor
  - Destructor

  If this was rust, .clone() would be an explicit call and a move
  wouldn't call any constructor or destructor, so the only things
  contained would be:

  - Size
  - Alignment
  - Destructor (Drop)

  We don't even have Drop yet, so the WVT only has to contain the
  type's size and alignment. Not much of a table heh...

  We'll have to do some kind of dictionary passing for the classes
  Cast, Num, Bitwise, and Ord I think.

  So for a polymorphic function, generate a single function that takes
  a reference to the value, a VWT (size, alignment), and dictionaries
  for any class constraints. In the generated code, use the VWT to get
  the size for when we need to allocate memory for the type, or
  memcpy. I'm thinking we won't need to though, right? Since it's
  already on the stack since it's behind a reference, we don't need
  the size for ~alloca~, and we only do store/load after a gep when
  indexing into the type, right? And that will only be done in
  monomorphic functions I believe.

  We must have what Swift calls "Metadata Patterns" as well. Say we
  have ~(define: (twice a) (Fun a [a . a]) (car (id [a . a])))~. We
  only pass the VWT of ~a~ to ~twice~, but we must also pass the VWT
  of ~(Pair a a)~ to ~id~, as well as the offset of the second element
  of the pair to ~car~. The second VWT and the rest of the metadata
  about the datatype must be constructed at runtime. So for every
  parametric datatype, we must generate a function that takes a VWT
  for each datatype parameter, and returns a /type metadata/
  value. The type metadata, beyond the VWT of the datatype, must also
  contain the offsets of each struct member.

  Metadata pattern example in Swift:

  #+BEGIN_EXAMPLE
  metadata pattern for Pair<T>   
  - first: T
  - second: T
  - value witness table

  metadata for Pair<Bool>
  - T: Bool
  - first: offset 0
  - second: offset 1
  - value witness table

  metadata for Pair<Int>
  - T: Int
  - first: offset 0
  - second: offset 4
  - value witness table
  #+END_EXAMPLE

  Generic member access in Swift:

  - Example:
    #+BEGIN_SRC swift
    func getSecond<T>(_ pair: Pair<T>) -> T {
        return pair.second
    }
    #+END_SRC
    
  - Implementation:
    #+BEGIN_SRC c
    void getSecond(opaque *result, opaque *pair, type *T) {
        type *PairOfT = get_generic_metadata(&Pair_pattern, T);
        const opaque *second =
            (pair + PairOfT->fields[1]);
        T->vwt->copy_init(result, second, T);
        PairOfT->vwt->destroy(pair, PairOfT);
    }
    #+END_SRC

  More things to consider when HOF:s are involved! https://youtu.be/ctS8FzqcRug?t=776

  Consider the case of a HOF accepting a monomorphic function. Something like:

  #+BEGIN_SRC carth
  (define: (apply f a)
      (forall (a) (Fun (Fun a a)
                       a
                       a))
    (f a))
  #+END_SRC

  Apply is a higher order function, and the type of the parameter ~f~
  is polymorphic (not higher ranked though). Therefore, in the lowered
  ~apply~, the lowered type of ~f~ will be something like
 
      void (*)(opaque *ret, opaque *arg, void *ctxt)
      
  What if we now have a simple, monomorphic function like ~neg~, of
  higher type ~(Fun Int Int)~. In the high domain, ~(Fun Int Int)~ is
  compatible with ~(Fun a a)~, but in the low domain,
  
      Int (*)(Int arg, void *ctxt)
      
  is not compatible with
  
      void (*)(opaque *ret, opaque *arg, void *ctxt)

  We thus need to generate an abstracting wrapper around concrete
  functions when passing them to a function that takes a non-concrete
  function as argument.

  Swift uses the terminology "Abstraction Patterns". "One formal type,
  many lowered representations". "Introduce thunks to translate
  between representations". To pass a concrete function as an abstract
  argument, they use what they call a "re-abstraction thunk". "We need
  to re-abstract the closure value, to match the abstraciton pattern
  of the function parameter. We do this using a thunk".

  The method itself is very obvious.

  #+BEGIN_SRC c
  Int closure(Int a) {
      return a + 1;
  }

  void thunk(Int *ret, Int *arg, void *thunk_ctxt) {
      Int (*fn_invoke)(Int, void*) = thunk_ctxt->...;
      void *fn_context = thunk_ctxt->...;
      ,*ret = fn_invoke(*arg, fn_context);
  }
  void *thunk_ctxt =allocate(..., closure, NULL);

  apply(..., thunk, thunk_ctxt, ...);
  #+END_SRC

* INACTIVE Use GADTs in Infer
* INACTIVE Add basic repl
  Add a basic repl based on the JIT. Something very similar to
  http://www.stephendiehl.com/llvm/.

  Could maybe be the starting point for an on-demand architechture?
  Would probably require some memoization mechanism so that we don't
  unnecessarily check, monomorphise, and compile stuff we don't need
  to.
* NEXT Type aliases
  Like ~type String = [Char]~ in Haskell.
* INACTIVE Query-based / on-demand compilation
  More or less a prerequisite to compile-time evaluation. Also enables
  good incremental compilation, and better IDE/LSP support.

  https://ollef.github.io/blog/posts/query-based-compilers.html
* INACTIVE Compile-time evaluation
  Could be used at different steps of compilation, for different purposes.

  - Procedural macros :: Can do more advanced generation.
  - Derive :: Using a similar mechanism to procedural macros, generate
    typeclass instances.
  - Conditional compilation :: If we for example allow comptime
    expressions evaluating to syntax at top level, we could use a
    mechanic similar to procedural macros for conditional
    compilation. Just have an if-expression on some compiler-defined
    global variable specifying e.g. what the platform is.
  - Dependent types :: Instead of having function and type-function
    definitions exist in separate spaces, like in Haskell, we could
    use normal functions. Could also use normal values, instead of
    having to redefine them at the type level (like having to define
    peano numbers and use datakinds in haskell).
  - Optimization :: Compute stuff att compiletime that can be computed
    at compiletime. Could probably use a mechanism similar to the
    dependent types to evaluate glob vars at compile time.

  Look at how zig, agda, and rust does it.

  Zig doesn't have macros -- their comptime only happens somewhere
  around the typechecking step. I think their comptime is evaluated by
  interpreting some mid-level IR. https://www.youtube.com/watch?v=8MbREuiLQrM

  Rust has constfn. Interpreting Miri.

  Agda idk.
  
  Query-based / on-demand compilation would make things *much*
  simpler, I'm fairly sure. Maybe even a prerequisite.

  proc-macros + parsing + mutual recursion seems like it might be a
  little tricky to solve. What if a proc-macro calls another
  proc-macro defined later in the file? Need to parse everything, so
  we can parse everything. Chicken and egg problem. Using Haskell
  laziness and ~fix~ might work. But the proc-macros don't just need
  to be parsed, but also typechecked and interpreted... Seems like
  tons of monadic complexity might surface.

  Do we do something like the typechecker, finding references and
  constructing a topological order of recursion groups ahead of time?
  Maybe use some kind of continuation-mechanism to exit parsing as
  soon as a proc-macro application is encountered, allowing resumption
  as soon as it has been defined?

  What about this: (direct or indirect) references to self must be at
  the "same level", i.e. you can't use self to generate the syntax of
  self, but you can call self as a normal (mutually) recursive
  function.

  So basically, if when doing query based compilation (which is depth
  first), and we reach a parsetime/macro application of self while
  still parsing self (i.e. it's in a stack of symbols of currently
  being parsed defs or smth), we return an error.

  Or maybe do like the typechecker and gather macro refs ahead of
  time. Like traverse the tree, and within all ~(parsetime ...)~ (or
  whatever) blocks, gather all referenced names. Do this for the while
  graph of referenced names recursively. In the end, we have a graph
  of all names necessary to parse the entry definition. Make a
  topological order. Compile them (to interpretable AST) in order. If
  there are any cyclical groups, compilation error.
* TODO Benchmark, profile, optimize
  Check out
  https://ollef.github.io/blog/posts/speeding-up-sixty.html. Great
  tips!
* INACTIVE Streamline learning the language
  Not that getting users is a primary concern, but if someone is
  indeed curious, I don't want them to be scared off by the process of
  getting started seeming complex.

  https://news.ycombinator.com/item?id=23347357
  https://www.hillelwayne.com/post/learning-a-language/
* NEXT Unify the different ASTs / IRs
  It's just kinda messy right now. Many files must be changed when
  touching just about any part of the AST representation. Also, takes
  up a lot of lines for not much apparent gain. Use some kind of
  attribute-tag to change the AST for different stages. Like:

  #+BEGIN_SRC haskell
  type Expr attr = Expr attr (Expr' attr)

  type ParsedExpr = Expr (Type, SrcPos)
  type CheckedExpr = Expr CheckedType
  #+END_SRC
* INACTIVE Builtin parsing of C header files
  I think Zig has this, and in Rust you can use the external tool
  ~bindgen~ to generate Rust declarations for C headers ahead of time.

  I just think it would be nice to not need to manually translate
  header files to use external libraries like OpenGL or SDL or
  whatever.
* INACTIVE Investigate alternative linkers
  Linking is one of the bottlenecks. However much caching etc I do in
  the parser & typechecker etc, the linker still has to do everything
  from scratch each time. I read somewhere that "gold" is a new GCC
  linker? Try using that maybe, unless it's already in use?

  https://news.ycombinator.com/item?id=24615916

  This is a new one: *mold*. It has as goal to be really fast. Seems promising!
  https://github.com/rui314/mold

* INACTIVE Produce .so:s for debug builds
  Linking is slow, so for debug builds we could try to split the
  output by module into separate .so:s. Then we'd only have to rebuild
  the .so of the affected module in incremental compilation.

  https://news.ycombinator.com/item?id=24615916

* INACTIVE Build Future into IO, or have both IO and AsyncIO?

* INACTIVE Union types
  Like Typescript (I think, I'm not all that familiar with it). Could
  be nice for error handling, for example. That's one of the problems
  in Rust -- you have to use all these fancy crates or write a bunch
  of boilerplate just to allow a function to return two different
  types of errors.

  Java, where exceptions can be combined as a union, essentially:
  #+BEGIN_SRC java
  public Foo foo() throws SomeException, OtherException {
      bar(); // throws SomeException
      baz(); // throws OtherException
  }
  #+END_SRC

  and Rust, where you have to combine the different types somehow:
  #+BEGIN_SRC rust
  fn foo() -> Result<Foo, MyErr> {
      bar().map_err(MySomeErr)?;
      baz().map_err(MyOtherErr)?;
  }

  enum MyErr {
      MySomeErr(SomeErr),
      MyOtherErr(OtherErr)
  }
  #+END_SRC
* INACTIVE Hygienic macros
* INACTIVE Destructors
  System to register a function as a destructor for a value, which can
  be used to destroy / close resources when the value is no longer
  used and garbage collection happens. It's not optimal that resources
  may stay open for quite a while after last usage, but it's better
  than *never* being closed.

  Example use case: We don't want to have to use linear types to
  manually destroy Lazy values when we're done with them, but we still
  need to make sure that their mutexes are destroyed at some point.

  https://www.hboehm.info/gc/finalization.html
* Pattern matching
** INACTIVE Var pattern syntax, comparison
  What if we did

  #+BEGIN_SRC carth
  (define (foo x pair)
    (match pair
      (case [x (let y)] (Some y))
      (case [_ _] None)))
  #+END_SRC

  instead of

  #+BEGIN_SRC carth
  (define (foo x pair)
    (match pair
      (case [x' y] (if (= x x')
                       (Some y)
                     None))))
  #+END_SRC
** INACTIVE Or-patterns
   Like in Rust. Very convenient.

   #+BEGIN_SRC rust
   match foo {
       (1, x) | (5, x) => x * 2,
       (_, y) => y,
   }
   #+END_SRC
** INACTIVE Active Patterns
   Like F# has. Something to
   consider. https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/active-patterns

   Could enable us to use pattern matching more?
   
* TODO Move from LLVM to alternative backend (MIR/QBE)
  LLVM is kind of not great in some ways. It's often not trivial to
  debug errors stemming from displeasing LLVM. It updates frequently,
  but the Haskell bindings lag behind, so I have to use an older
  version or start maintainin llvm-hs myself. The project is
  *massive*, and most of the stuff I don't need. Sure, it's nice being
  able to target practically any backend, but I don't *actually* care
  about most of them. And there exists *so many* optimization passes,
  but most of them actually improve the performance of the binary very
  little, while bumping the compiletime a not insignificant bit.

  I want to use something simpler.

  To make the transition smooth, and to allow for easier debugging of
  codegen in the future, I think it would be a good idea to add an
  interpreter, like the one we had before, but now supporting FFI
  calls so that std-rs can be used as well. Really, the amount of code
  would not be huge, and it would be incredibly nice to have something
  to compare to when debugging low-level stuff. Also, I want to get
  rid of LLVM right away, but I'm not sure about what to replace it
  with just yet, so an interpreter is needed in the meantime.

So we're done implementing Lower and Low, so the next step is simply
to add additional backends and remove LLVM!

Investigate QBE, Cranelift, GNU Lightning, libgccjit, GCC, MIR.

#+BEGIN_QUOTE Candidates
- C :: I.e., spit out C source and call out to ~cc~. Very portable
  (every platform has a C compiler). Not very elegant. Does not
  natively support tail call elimination, so would have to do that
  myself (true for pretty much everything except llvm though). Used
  by respectable languages like Nim and Haskell (sort of).
- C-- :: Similar to C, but even more "portable assembly
  language". Created by SPJ and friend, specifically for being
  generated by compilers. Fork called Cmm used by GHC.
- LLVM :: Approx 5 million LOC. Many targets, OK usability, but
  breaking changes sometimes and big and scary.
- GCC :: Even bigger than LLVM. Also many targets. Not very good
  usability. Probably quite stable. GPL.
- libgccjit :: Despite the name, also AOT. Basically an easier to
  use frontend for GCC with additional functionality to leverage GCC
  for JITting. Most points of GCC apply, but easier to use, and JIT
  included.
- GNU Lightning :: JIT (only). Used by some schemes. Disjoint from
  GCC.
- Cranelift :: Small-ish atm, but not sure it has any goals to stay
  that way. Seems more like an effort to replace LLVM, including
  much of its "bloat". Written in Rust. Maybe not all that
  standalone? Seems to be meant to be called from Rust. Performance
  of generated code seems bad atm, but should be improved.
- QBE :: Small! 10k LOC. Goals to be 70% as fast as
  GCC/LLVM. Generates ASM instead of machine code for some
  reason. Seems like it hasn't seen much update this last
  year. However, one [[https://github.com/michaelforney/qbe][Michael Forney is actively maintaining a fork]],
  for his own language I think, so that might be interesting.
- [[https://github.com/vnmakarov/mir][MIR]] :: This one looks the most interesting! Similarly to QBE, very
  small at 15k LOC and 70% the performance of GCC. Primarily a
  JIT(?), but seems to be able to to AOT as well. Has a 4 backends
  atm, including AMD64 and Aarch64, and it seems relatively easy to
  add a new one. I've found 2 languages that make use of MIR to
  study: [[https://github.com/grame-cncm/faust][Faust]] and [[https://github.com/dibyendumajumdar/ravi][Ravi]].
#+END_QUOTE

In the end, I most like the look of MIR. It seems to make good
tradeoffs.

Compiling to C comes at second place. Incredibly portable, and .c
files would be a lot more readable than .ll files. Would lose the
GDB source-line from DWARF stuff though, but that shit kinda sucked
anyways. Function names would work as well, if not better than in
LLVM, since the names would be kept in the C, and C compilers
probable output much better dwarf than I ever could.

Maybe I'll do both? If I just a low-level IR that's just above the
level of the union of C and MIR it ought to be quite simple to
translate from that to whatever backendest backend.

Ravi, a language using MIR: https://github.com/dibyendumajumdar/ravi

** References
   - [[https://gist.github.com/zeux/3ce4fcc3a43072b4315abde95319ecb6][How does clang 2.7 hold up in 2021?]]
* TODO `tail` keyword to ensure tail call or compiler error
  Sometimes you want to be sure that tail calls are optimized. To be
  able to assert this at compile time, so as to not accidentally
  create a stack consuming function when it really matters, add a
  `tail` keyword.

  TCO should already performed as an optimization, but with `tail`,
  you can ensure that you get a compiler error if the call is not
  actually a tail call, if you've done something wrong or
  something. Sort of like Rust is considering the `become` keyword to
  work?
* TODO Stack traces
I want something like what you get when running a panic:ing Rust
program with ~RUST_BACKTRACE=1~.

Annotating generated code with good DWARF is not easy, and it wouldn't
be optimally useful in an expression oriented language anyways. That's
more for line-by-line / statement oriented languages like C. But
stacktraces are awesome! Seeing exactly where a panic occurs is
critical for quick debugging.

So how do we get the traces? Maybe using some kind of shadow
stack. Such a method might work well for other potential backends as
well, like a C backend.

* INACTIVE Add kind of ~apply~ function that takes tuple
  #+BEGIN_SRC carth
  (define (foo a b c)
    (+ a (* b c)))

  (assert-eq (foo 1 2 3) (apply foo [1 2 3]))
  (assert-eq (foo 1 2) (apply foo [1 2]))
  #+END_SRC

  In general, ~(apply f [x1 ... xn])~ becomes ~(f x1 ... xn)~.

  I think it could be a function, via a type class instance that
  recurses on the pairs of a tuple.

  One usage that could be nice in particular is when you want to apply
  a function with "default" arguments. You could then do ~(apply f
  default)~ instead of anything more complex.

  Then again, you can do something arguably more convenient with
  typeclasses and deriving in haskell. Create a record for the
  specific argument set, derive Default, and call it like ~f (default
  {foo = 3})~.
  
* INACTIVE SoA record attribute
  https://blog.royalsloth.eu/posts/the-compiler-will-optimize-that-away/

  Convenient syntax for using SoA/AoS could be nice for lowe level
  stuff, or we might consider it too seldom an issue for a somewhat
  high-level languge like Carth.
* INACTIVE Recursion schemes
  Recursion schemes are functions that capture patterns of recursion,
  like fold and unfold. These 2 are simple to implement. Other
  schemes, less commonly used yet frequently applicable, like cata,
  could be implemented as well, but might require some built in
  support or smart "deriving".

  Look at https://hackage.haskell.org/package/recursion-schemes-5.2.2.1

  Maybe deriving functor and/or foldable could include this base
  functor thingy?

* INACTIVE Borrow checking
  Don't think I'll implement anything like this. There's Carp or Rust
  or whatever if you prefer that. I kind of want a nice GC actually.

  But anywho, in case we ever want to add borrow checking, I'll
  collect some useful notes here.

  Check out Polonius, the new borrow checker in Rust. https://youtu.be/H54VDCuT0J0

** TODO Dead code elimination of externs & wrappers
   We already do dead code elim almost by mistake in Monomorphize, but
   we still generate declarations and wrappers for all
   ~extern~:s. Getting rid of them would be nice.
   
* INACTIVE GPU targetable
  Either in Carth directly, or via a DSL or something. Some method of
  doing flattening and parallelisation like Futhark? Compile to OpenGL
  & Vulkan maybe.

* NEXT Write c compiler i carth
  Look at tutorials. There are many minimal c compilers. tinycc(?) is one, IIRC.

  At first, just a fun exercise. Seeing how well Carth fares at such a
  task. Discovering new bugs & limitations of the compiler. Coming up
  with new features.

  In the future, may be integrated in a self-hosted Carth compiler for
  C header parsing support, or even full-on C source library
  support. Kind of like Zig.

* NEXT Sugar for lambdas
  Look at [[https://clojure.org/guides/learn/functions#_anonymous_function_syntax][Clojure's reader shorthand for anonymous functions]].

  It's basically De Brujin notation. So ~(fn [a b] (* 5 (+ a b)))~ can
  also be written ~#(* 5 (+ %1 %2))~. That's convenient! If one
  instead does good point-free compositioning, like ~(<oo (* 5) +)~,
  the sugar is "unneccesary", but it really is quite concise and
  readable. Might be nice to have.

* NEXT Look at these languages
  For inspiration, learn from their mistakes, etc.
  
  Also add related work to readme, after looking closer at it, if applicable.

  - Hackett
  - Liskell
  - Axellang
  - Kalyn
    https://intuitiveexplanations.com/tech/kalyn#preliminary-technical-design-decisions
  - Unison
    https://github.com/unisonweb/unison
* NEXT Refactor type checker
  keywords: type checking, inferenc, inferrer

  I'm not completely happy with the typechecking. 4 module files
  (Check, Checked, Infer, Inferred) totalling over 900 SLOC. Also,
  ~solve~ is not just run once at the outermost level, visiting each
  constraint at most once. Because of nested ~let~ with polymorphism,
  we currently run ~solve~ nestedly, and in total, each constraint is
  likely visited more than once. This is ugly.

  See:
  - https://gilmi.me/blog/post/2021/04/06/giml-type-inference

* INACTIVE Nonstrict parameters
  Similar to how you can mark parameters as strict and force
  evaluation in Haskell, we could benefit from having params marked as
  nonstrict similarly.

  Then we can write functions that perform some sort of
  short-circuiting logic, like ~or~, ~parse/or~, ~maybe/or~, ~maybe~,
  etc, without having to resort to macros or explicit wrappings of
  ~(fun (Unit) ...)~s.

  It could look something like this
  
  #+BEGIN_SRC carth
  (define (or p #nonstrict q)
    (if p True q))

  (or (cheap-computation ...)
      (expensive-computation ...))
  #+END_SRC

  Also consider the nested case

  #+BEGIN_SRC carth
  (define (foo a b #nonstrict computation)
    (if (bar a)
        (baz b computation)
      3))

  (define (baz b #nonstrict computation)
    (or b (f computation)))
  #+END_SRC

* INACTIVE Better unicode support
  Possibly using Rust's builtin stuff. Also possibly use some Zig library?

  Otherwise, this Suckless library seems quite nice: https://libs.suckless.org/libgrapheme/

  Very small! That's always a plus :)

* INACTIVE Dynamic dispatch
  Like Box<dyn TRAIT> in Rust. Might be useful in places. Should not
  be that hard to implement -- just heap allocate a vtable, and
  populate it with all of the class functions. Might need to add
  wrappers so that the functions always accept the type by reference?
  Or all args by reference? Unless we modify the compiler to *always*
  pass args by reference. In Rust, I suppose they defer the problem by
  only allowing one to call ~&self~ and optionally ~&mut self~ methods
  on a trait objects. Don't have to consider sizes if you can't even
  call ~self~ methods in the first place.

  Must consider how this interacts with monomorphization vs. boxing
  vs. value witness tables for static dispatch.b
* INACTIVE Have error messages quote section numbers for the spec
  when there is a spec.

  Would be nice, to have concrete documentation for what is ok and what is not.
* NEXT Dump everythiong to Graphviz
  Particularly the pre-LLVM ASTs. They're very hard to read as text,
  but would probably fit really well as a graph. This could be useful
  both for debugging the compiler, as well as to debug compiled
  programs.
* TODO Change syntax of definitions & funs
  #+BEGIN_EXAMPLE
  ;; I don't like how it's only the parmeters in parens in fun
  ;; and Fun, but in define the function name is included as well.
  (Fun      (a b) c)
  (fun      (a b) c)
  (define (f a b) c)

  ;; This would align up much more nicely. More symmetry == better.
    (Fun   (a b) c)
    (fun   (a b) c)
  (defun f (a b) c)
  #+END_EXAMPLE

  No but really, more symmetry => easier for beginners to make correct
  extrapolations => less friction.

  Also, it's slightly annoying having to jump back to the ~define~
  just to make it a ~define:~ when we want to add a type signature to
  a definition. Maybe instead:

  #+BEGIN_EXAMPLE
  (defun f (a b) : (Fun (a b) c)
    c)
  #+END_EXAMPLE

  Also, since fun:s are essentially treated as a special case of
  fmatch:es in the frontend, we should support matching immediately in
  the definition, I'm thinking. Just a bit of extra sugar.

  #+BEGIN_EXAMPLE
  (defun f (a b)
    (* a b))  
  (defvar f
    (fun (a b)
      (* a b)))

  (defun* f
    (case (a True)  (* a 10))
    (case (a False) (+ a 1)))
  (defvar f
    (fun* (case [a True]  (* a 10))
          (case [a False] (+ a 1))))
  #+END_EXAMPLE

  Something like this? Would help reduce rightward drift, and would be
  a fairly common pattern, I believe.

  If we have a defun and defun*, I think we should have a fun and fun*
  as well. That would essentially entail renaming fmatch to fun*.

  fun*/fmatch lives somewhere between both fun and match. I don't
  think that leaning towards one or the other is more of a correct
  choice, so I feel we can just pick something arbitrarily.

  I used to like fmatch, because it matches \case in Haskell, but now
  I'm leaning more towards fun*. fun* is particularly nice if we go
  with defun* as well. Nice symmetry.

  Another approach, which I've pretty much already rejected, is to
  merge fun and fmatch into a single, overloaded fun. Might be nice in
  some ways, but it might also get kinda confusing. More ambiguity
  etc. Not as trivial to parse in carth-mode, for indentation etc. No,
  better to have different special forms, I think.
