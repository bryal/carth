#+TITLE: The Carth programming language

Features and other stuff to do/implement in/around Carth.

* INACTIVE Package system
  We will need some kind of package-management solution, or we will
  end up like C where using dependencies is a chore.

  I kind of want to just use Guix somehow, but it should ideally be
  possible to use whatever package management solution on other
  distros as well...

  I'm thinking: the package manager shouldn't make too many
  assumptions about the system and get in the way in unorthodox
  scenarios. Consider how Guix and Nix have to use some hacks to get
  packages to play nice with the store, not make assumptions about the
  dir tree structure (like there being a /usr/lib, a /bin/bash) and
  stuff like that.
  
  See: https://drewdevault.com/2021/09/27/Let-distros-do-their-job.html
** Carth + Guix = üíú?
   Anv√§nd guix som pakethanterare + virtualenv f√∂r Carth, typ som Stack
   f√∂r Haskell och Cargo f√∂r rust.

   T√§nk att carth antingen drar in guix som bibliotek, eller p√• n√•got
   annat vis kr√§ver guix fr√•n systemet.

   T√§nk:
     carth init
   Skulle kunna k√∂ra
     guix environment
   och lite mer.

   Update <2021-10-20 ons>: I think pulling in guix as a dependency
   would not work well. That's not how it's designed to be used. Would
   also work weirdly when actually using Guix as the OS package
   manager on top as well. Also, asking users to install Guix and add
   our channel would not be ergonomic enough to be the simplest way to
   do things.

   Instead, I think we should do something like what this comment
   proposes: https://news.ycombinator.com/item?id=24847060, i.e.,
   imitate cargo. Cargo is fairly primitive, and maybe allows "too
   much" when it comes to using different versions, but it's fairly
   simple and does its job well. Then we can "bless" Guix as a package
   manager to use on top of our primitive cargo-clone for more
   advanced features, like releases with compatible packages,
   environments, etc etc. Basically, we don't put too much
   responsibility on our own shoulders, and let OS distributions do
   what they do best. Make sure that cargo-clone never gets in the way
   of Guix and other distributions. For example, overriding dependency
   config to use local files and other versions should have first
   class support.

   Maybe help maintain the Guix (& Nix?) packages myself.
   
*** Extra s√§kert?
    "guix environment allows you to run any application in an isolated
     container that restricts network access, hides the filesystem (no
     risk that the closed-source program steals any of your files, say
     your Bitcoin wallet or your PGP keys) and even system-level
     information such as your user name. This is essential to run any
     non-trusted, closed-source program."

    Skulle kanske f√∂rhindra vissa av problemen man sett i NPM, typ "event-stream" debaklet.

*** L√§s mer
    https://www.gnu.org/software/guix/manual/en/guix.html#Channels
  
* NEXT Module system
  Postfix syntax for module paths? A bit like web-domains -
  "sub.main.top". E.g. "vector.collections.std".  Most relevant
  information floats to the left. Maybe a good idea, maybe
  not. Consider it.

  Look at ML modules.

** INACTIVE Allow conflicting imports if unambiguous?
   I'm thinking something that would allow the following. It would be
   less annoying than having to qualify everything. Also, gotta think
   about how this relates to overloading √† la C++.

   #+BEGIN_SRC carth
   (module Foo
           (data FooThing First Second)
           (define: isFirst
               (Fun FooThing Bool)
             (fun-match
               [First True]
               [Second False])))

   (module Bar
           (data BarThing First Second)
           (define: isFirst
               (Fun BarThing Bool)
             (fun-match
               [First True]
               [Second False])))

   ;; First, there should be no error for just importing modules with conflicting
   ;; defs. This is ok in Haskell, unless one of the conflicting defs is used.
   (import Foo)
   (import Bar)

   ;; Second, it should be allowed to use one of a set of conflicting defs if the
   ;; type makes it unambiguous....

   ;; either explicitly
   (define: x FooThing First)
   (define: y BarThing First)

   ;; or implicitly
   (define t (isFirst x))
   (define u (isFirst y))
   #+END_SRC

* NEXT Typeclasses
  Need some kind of system like type classes for ad hoc
  polymorphism. Maybe Haskell style type classes, Agda style
  implicits, or Ocaml style modules. Not sure.

  "Type classes are functions from types to expressions"
  https://youtu.be/5QQdI3P7MdY?t=920. Interesting thought! Can we view
  type families the same way, but functions from types to types or
  smth? Maybe we can come up with more intuitive terminology.

  https://www.microsoft.com/en-us/research/wp-content/uploads/1994/04/classhask.pdf
  https://static.aminer.org/pdf/PDF/000/542/781/implementing_type_classes.pdf

** Agda style classes w implicit args
   https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#implicit-parameters

   In Haskell, you can only have a single instance of a specific
   typeclass for a specific type. This doesn't always make
   sense. Consider Semigroup for Int. Both + and * make sense, but we
   can only have one unless we goof around with newtypes etc, and that
   kinda sucks.

   Consider an approach more like agda. That model is more lika basic
   Hindley-Milner + dictionsry passing, except the "typeclass"
   argument can be passed implicitly with the {} syntax! That seems
   really cool.

   I'm not sure how implicit arguments work though. Does the compiler
   just look at all available bindings and pick the first/only
   available variable of that type?

   https://agda.readthedocs.io/en/v2.5.2/language/implicit-arguments.html

   https://agda.readthedocs.io/en/v2.5.2/language/instance-arguments.html

   Or just do it kind of Haskell style, but give the instances names
   and allow multiple, overlapping instances, raisi g an error if the
   instance is ambiguous somehow.

   Problem with instances as implicit arguments:
   https://youtu.be/2EdQFCP5mZ8?t=1259.  We'd have to know exactly
   which instances exist for the same type, and from where they're
   imported and what scoping they'll have. That sucks. Another
   horrible thing: imagine creating a sorted list with one instance, and doing
   a sorted lookup with another (accidentally or not), you could an incorrect
   result with no error from the compiler!

   Maybe an alternative could be to have both ~primary~ and
   ~secondary~ instances, where the primary instances may not overlap
   or be orphaned, like Rust, but may be passed implicitly, while
   secondary instances may overlap and be orphaned, but must be
   "overriden"/passed explicitly.

   But that may also not work. For the following code,

   #+BEGIN_SRC haskell
   foo :: Foo a => a -> a
   foo = bar

   bar :: Foo a => a -> a
   bar = ...
   #+END_SRC

   consider that we call ~foo~ with an explicit secondary
   instance. What instance will ~bar~ be given? If we must pass
   secondary instances explicitly, it seems ~bar~ would get the
   primary instance, and ~foo~ and ~bar~ would be called with
   different instances. BAD!

   Probably last update for this section: [[https://old.reddit.com/r/haskell/comments/765ogm/multiple_type_class_instances_for_the_same_type/][this thread]] has convinced me
   that Haskell-/Rust-style typeclasses is the best idea.

* NEXT Linear types
  Linear types would allow predictable performance and behaviour of
  e.g. IO tasks. Force a single manual file-close or
  buffer-flush. Force a single free for malloc.  Affine types would
  allow better performance.  E.g. pure, in-place modification of
  array.  If noone else points to it, value can be consumed and
  modified rather than cloned. Something like: ~fn push(mut v:
  Vec<i32>, x: i32) -> Vec<i32> { v.push(x); v }~ Implemented as maybe
  a wrapper, or an interface?  Maybe like in haskell with lolly
  operator?

  Things to consider: Linear arrow vs. `kind` approach or similar?

  Check out Idris Uniqueness types, Linear Haskell's linear arrows,
  and however Blodwen does it (linear arrows kind of I think).

* NEXT Higher kinded types

* INACTIVE Type families / functional dependencies and multi-param classes / Dependent types
  I'm on the fence here, but the consensus seems to be that type
  families are better than fundeps. Also, it might be possible to
  avoid needing to implement Multi-parameter typeclasses if type
  families are available to compensate. Seems that would reduce
  ambiguities and mental overhead a bit.

  Neither type families or fundeps are necessary if we have dependent
  types, but that would likely bring difficulties of it's own.

  Type families in Haskell vs Dependent types in a pseudo-Haskell vs
  Dependent types in Agda:

** Sketch
   The wiki page is
   good. https://en.wikipedia.org/wiki/Type_family. Haskell wiki also
   has some interesting notes
   https://wiki.haskell.org/GHC/Type_families.

   https://en.wikipedia.org/wiki/Lambda_cube

   Does it complicate typechecking? It's not obvious to me how it
   would?

   In haskell, type families and data families are always
   open. Probably fine to keep it that way? Not sure the complexity of
   having both open and closed versions are worth it?

   Relations:
   - Function :: Value -> Value
   - Typeclass :: Type -> Values
   - Typefamily :: Type -> Type
   - Dependent type :: Value -> Type

   I don't love the names "family" and "class". Could we use something
   that makes more clear the relations above? Like "type function" or
   something? Although, I guess at least "class" wouldn't be so bad to
   keep, for familiarity reasons.

   Do we need data families as well? I'd prefer not to have to add
   them also. A little bit of inconvenience remaining is worth it if
   we can avoid a lot of complexity in the language.

   Observation: Type families are just type aliases, but we can
   pattern match on the input.

   Observation: A typeclass with associated types is basically an
   extension of normal typeclasses that makes it (Type -> (Type,
   Value)). Defining an associated type in an instance of a typeclass
   is basically a way of allowing one to add cases to the pattern
   matching after definition. Consider this:

   #+BEGIN_SRC carth
   (type (Foo a)
     (Match a
            (case Bar Int)
            (case Baz Bool)))
   #+END_SRC

   this is the same as

   #+BEGIN_SRC carth
   (class (Foo' a)
     (type (Foo a)))

   (instance (Foo' Bar)
     (type (Foo Bar) Int))

   (instance (Foo' Baz)
     (type (Foo Baz) Bool))
   #+END_SRC

   The difference being that with the typeclass version of
   typefamilies, cases/definitions can be separated from the
   declaration, and user modules can extend the type family by adding
   another instance.

   #+BEGIN_SRC carth
   ;; Warning: some pseudocode and unimplemented features

   ;; The different possible forms, which would be basically
   ;; equivalent. Each could be convenient, but not sure if
   ;; it's a good idea to implement all.

   ;; Single case

   ;; Alias form
   (type (Option a) (Maybe a))

   ;; <=> closed case form
   (type (Option a)
     (case (_) (Maybe a)))

   ;; <=> open case form
   (type (Option a))
   (type case (Option _) (Maybe a))

   ;; <=> class form
   (class (Foo a)
     (type Option))
   (class case (Foo a)
          (type Option (Maybe a)))


   ;; Multiple cases

   ;; Can't be described as alias
   ...

   ;; closed case form
   (type (Result ok err)
     (case (_ Unit) (Maybe ok))
     (case (_ _)    (Either err ok)))

   ;; <=> open case form
   ;;
   ;; Unlike value pattern matching, order shouldn't matter, as
   ;; we could be defining each case in a different
   ;; package. Some other algorithm for handling overlapping
   ;; instances would have to be used.
   (type (Result ok err))
   (type case (Result ok err)  (Either err ok))
   (type case (Result ok Unit) (Maybe ok))

   ;; <=> class form
   (class (Foo ok err)
     (type Result))
   (class case (Foo ok err)
          (type Result (Either err ok)))
   (class case (Foo ok Unit)
          (type Result (Maybe ok)))
   #+END_SRC

   Typeclass (Type, Values) vs Type family + normal typeclass:

   #+BEGIN_SRC carth
   ;; 1

   ;; should implicitly create namespace `Iter`, so it's `Iter/Item` and `Iter/next`
   (class (Iter it)
     (type Item)
     (: next (Fun it (Maybe [Item it]))))

   (class case (Iter (Array a))
          (type Item a)
          (define (next arr) ...))

   ;; 2
   ;; <=> (except for namespacing)

   (type (Iter-item it))
   (type case (Iter-item (Array a)) a)

   (class (Iter it)
     (: next (Fun it (Maybe [(Iter-item it) it]))))

   (class case (Iter (Array a))
          (define (next arr) ...))
   #+END_SRC

   And in real Haskell that compiles, for comparison:

   #+BEGIN_SRC haskell
   -- 1

   class Iter i where
       type Item i
       next :: i -> Maybe (Item i, i)

   instance Iter [a] where
       type Item [a] = a
       next = \case
           [] -> Nothing
           a : as -> Just (a, as)

   -- 2

   type family Item' i
   class Iter' i where
       next' :: i -> Maybe (Item' i, i)

   type instance Item' [a] = a
   instance Iter' [a] where
       next' = \case
           [] -> Nothing
           a : as -> Just (a, as)
   #+END_SRC

   https://blog.rust-lang.org/2021/02/11/Rust-1.50.0.html#a-niche-for-file-on-unix-platforms

** Type families, Haskell
   #+BEGIN_SRC haskell
   class Iter c where
       type Item c
       next :: c -> Maybe (Item c, c)

   nextList :: [a] -> Maybe (a, [a])
   nextList = \case
       [] -> Nothing
       a : as -> Just (a, as)

   instance Iter [a] where
       type Item [a] = a
       next = nextList
   #+END_SRC

** Dependent types, pseudo-Haskell
   #+BEGIN_SRC haskell
   class Iter c where
       item :: Type
       next :: c -> Maybe (item, c)

   nextList :: [a] -> Maybe (a, [a])
   nextList = \case
       [] -> Nothing
       a : as -> Just (a, as)

   instance Iter [a] where
       item = a
       next = nextList
   #+END_SRC

** Dependent types, Agda
   #+BEGIN_SRC agda2
   record Iter (C : Set) : Set1 where
     field
       item : Set
       next : C -> Maybe (item √ó C)

   nextList : {A : Set} -> List A -> Maybe (A √ó List A)
   nextList [] = nothing
   nextList (x ‚à∑ xs) = just (x , xs)

   listIter : {A : Set} -> Iter (List A)
   listIter {a} = record
     { item = a
     ; next = nextList
     }
   #+END_SRC

* INACTIVE Custom GC
  Until we get linear types, and even then, we'll need some form of
  GC. Boehm's seems to be working well enough, but a conservative
  collector is not ideal, and I think it would be a fun project to
  write my own GC.

  There are many problems with refcounting: Generated llvm ir/asm gets
  polluted; While performance is more predictable, it's typically
  worse overall; Cycle breaking would either require using weak refs
  where appropriate, which would in turn require user input or an
  advanced implementation, or a periodic cycle breaker, which would be
  costly performance wise. So tracing GC is probably a good idea.

  GHC seems to prefer throughput over latency, so very long pauses are
  possible when you're working with a nontrial amount of data. "You're
  actually doing pretty well to have a 51ms pause time with over 200Mb
  of live data.".

  It could be interesting to add ways of controlling when GC happens
  so you can reduce spikes of latency. Haskell has ~performGC :: IO
  ()~ that does this. [[https://old.reddit.com/r/haskell/comments/6d891n/has_anyone_noticed_gc_pause_lag_in_haskell/di0vqb0/][Here is a gameboy]] who eliminates spikes at the
  cost of overall performance by calling ~performGC~ every frame.

  [[https://github.com/rust-lang/rfcs/blob/master/text/1598-generic_associated_types.md][Some inspiration here]].

  A tracing GC would be quite separate from the rest of the
  program. The only pollution would be calls to the allocator (not
  much different from the current sitch w malloc) and
  (de)registrations of local variables in Let forms (a total of two
  function calls per heap allocated variable).

  Implementing a tracing GC would be a fun challenge, and I'm sure it
  could be fun to try different algorithms etc.

  Look at
  - https://github.com/mkirchner/gc
  - https://youtu.be/FeLHo6tIgKI

* INACTIVE Effect system
  tags: Algebraic effects
  
  Seems like it could be more elegant than monad transformers,
  although maybe not as fast?

  Effect fusion seems to make it faster?

  Read Wu, Schrijvers 2014, 2015, 2016. I think their papers basically
  present the concept of fused effects.

  github.com/fused-effects/fused-effects

  https://youtu.be/vfDazZfxlNs?t=1730

  ^ det makear sense. Bygg basically upp ett tr√§d av den h√§r datatype,
  och interpreta det med alla handlers. Varje handler kollar om det √§r
  dens variant, och isf k√∂r effekten. F√∂r varje handler blir tr√§det
  simplare, och till sist √§r det bara Pure kvar.

  Naiv implementering ineffektiv. Bara t√§nk -- m√•ste interpreta ett
  tr√§d ist f√∂r att bara *g√∂ra* effekterna direkt!

  Man kan anv√§nda free monads f√∂r att bygga upp tr√§det, men detta √§r
  inte s√• effektivt.

  Grundid√©n med papret "fusion for free" √§r att man vill bara traversa
  tr√§det en g√•ng, och inte en g√•ng per effect handler.

  Med "fusion" verkar de syfta p√• funktionaliteten i GHC, att man kan
  fusionera ihop funktionsanrop av specifika m√∂nster till mer
  effektiva varianter. E.g., ~map f . map g~ fusioneras till ~map (f
  . g)~. P√• liknande vis fusioneras ~fold handleState . build . fold
  handleReader~ till bara ~fold (handleState . handleReader)~. Kan vi
  l√∂sa detta utan kompilatorst√∂d, eller √§r det kanske v√§rt att l√§gga
  till?

  See the talk on polysemy, it's a good complement and alternative to
  the fused effects one. https://youtu.be/-dHFOjcK6pA.

  We need type-level lists or sets, and a way to implement Member on
  that thing. If tuple types could contain higher kinded types, I
  think we only need classes.

  See:
  - https://youtu.be/z8SI7WBtlcA, https://youtu.be/z8SI7WBtlcA?t=1433
  - Eff language
  - https://youtu.be/XAnFUwIaZB8

** INACTIVE Memory allocation as an explicit effect
   In Rust, you can override the global memory allocator. Situational
   override is not really possible? I think either you use the global
   allocator, or you allocate with e.g. an arena explicitly.

   In Zig, all allocation is explicit, and you have to pass around
   whichever allocator you want the functions to use. Pro: easy to
   override allocation for an object or sub-program with e.g. an
   arena. Con: verbose, bothersome, less convenient.

   Maybe we could make heap allocations sort of semi-explicit in
   Carth, via an Effect system? Easy to override with e.g. arena
   allocator for specific functions, and not as inconvenient as
   Zig. Do-notation (or better? (like generalized application)) could
   make it fairly convenient, and there really is some usefulness to
   doing it. Would encourage keeping things on the stack whenever
   possible. But maybe it's too much inconvenience for a high-level
   lang? I mean, couldn't pretty much any closure actually heap
   allocate for the captures? Hmm.
  
* INACTIVE Property system
  I'm thinking of a system where you annotate functions in a source
  file with pre- and postconditions, which can then be checked in
  different modes depending on how much time you've got etc.

  - Proof-mode. Exchaustive checking of conditions. All possible
     inputs are generated, and the system checks that the precondition
     always implies the postcondition.
  - Test-mode. Statistical, random testing. Generate enough inputs
    such that the precondition is fulfilled for a statistically
    significant subset of the complete set of possible inputs.
  - Debug-mode. Functions are not tested ahead of time, instead
     assertions are inserted and checked at runtime.
  - Release-mode. Conditions are completely ignored.

* NEXT Consider using lib for pretty printing
  https://hackage.haskell.org/package/pretty-1.1.1.1

* INACTIVE Hoogle equivalent
  https://wiki.haskell.org/Hoogle

* INACTIVE Playground
  Like play.rustlang.org

  https://play.rust-lang.org/help
  https://github.com/google/nsjail

  Might actually be pretty easy by making use of Guix
  containers. Sandboxes the filesystem, and doesn't give network
  access unless `--network` is provided.

  #+BEGIN_EXAMPLE
  guix environment --container --ad-hoc coreutils clang carth
  #+END_EXAMPLE
* INACTIVE Language server protocol
  [[https://github.com/Microsoft/language-server-protocol]]
  [[https://internals.rust-lang.org/t/introducing-rust-language-server-source-release/4209]]

* INACTIVE HTML documentation generation
  Like [[https://www.haskell.org/haddock/][haddock]] and [[https://www.haskell.org/haddock/][rustdoc]].

* INACTIVE Documentation checker
  Like a typechecker-pass but for generated documentation. Verify that
  all links are alive, that examples compile and produce the expected
  output, etc.
* Standard library (std, stdlib)
  Prefer somewhat big / wide stdlib. Small / bad standard library +
  good package manager => npm / cargo situation, where everything has
  sooo many dependencies. Having a dep is not bad per say, but when
  the numbers completely blow up, like in rust- and javascript-land,
  things can get messy. The best way to avoid this, I think, is having
  a standard library that has you covered for most common things.

  Examples of libraries in other ecosystems that should be part of the
  stdlib: `is-even` in JavaScript, `composition` in Haskell, `rand` in
  Rust.

  Go seems to have done this relatively well. Their stdlib has
  everything from JPEG codec, to a webserver. The stdlib shouldn't
  have everything though, as that will add a bunch of legacy cruft
  over time, like in Java. Would not be as much of a problem if we're
  not afraid of releasing new major versions removing deprecated
  stuff.

  Maybe separate stdlib into core and std. Core could be a smaller
  subset which is pretty much purely implemented in carth, so it's
  easy to use with interpreter and comptime. Conditional compilation
  to use efficient C/Rust versions normally.

** INACTIVE Numbers, algebra, mathematics
   How to best structure the numeric typeclasses? ~Num~ in Haskell is
   a bit coarse. For example, you have to provide ~*~, which doesn't
   make much sense for ~Vec3~, so you can't give a proper instance for
   ~Vec3~ to get ~+~. Maybe [[https://hackage.haskell.org/package/numeric-prelude-0.4.3.3][numeric-prelude]] could be a good
   alternative to look at?

   [[https://typeclasses.com/featured/to-integral-sized][toIntegralSized]]
*** INACTIVE Division of integers should return Rational?
    Lossless etc. No truncation by accident. SBCL LISP does this I think?

    Consider type size and overflow though. Maybe only do this for
    arbitrary-sized Integer, and not for fixed-sized Int.
** INACTIVE Concurrency / parallelism primitives
   Mutex, semaphore, etc.

   Look at how Rust and Haskell do it.

   Also, look at the crate [[https://crates.io/crates/parking_lot][parking_lot]], which does replaces the
   standard Rust primitives with smarter ones. E.g. the mutex does a
   small number of spins first, to avoid expensive thread juggling by
   the OS when the critical section is very short, but resort to the
   usual process interrupts in case it goes on for longer, to avoid
   priority inversion which is a problem with spinlocks.
   https://matklad.github.io/2020/01/02/spinlocks-considered-harmful.html
   https://matklad.github.io/2020/01/04/mutexes-are-faster-than-spinlocks.html

   Lock Free Data Structures using STM in Haskell: https://www.microsoft.com/en-us/research/wp-content/uploads/2006/04/2006-flops.pdf

** INACTIVE Random number generation
   References:
   - [[https://arxiv.org/abs/1910.06437][It is high time we let go of the Mersenne Twister]]
* NEXT Some algorithms & data structures
  We need good collections & algs for sorting etc. if Carth is going
  to be of any use to anyone. Would also be a good way to add to the
  set of test-programs & find the worst pain points of current Carth.

  Many of these have implementations to look at and compare to on
  [[rosettacode.org]].

  This list is sort of off the top of my head, so some might not be
  good fits in a purely functional language. Look at some resource on
  persistend data structures as well.

  - Priority queue
  - Binary tree
  - B-tree
  - Random number generator
  - Binary search
  - bubble, insertion, selection sort
  - quicksort

* INACTIVE "Global" memoization
  This is just an idea I had, and may or may not be wise to implement.

  Add a special function for "memoized application" that acts like the
  application function (in Haskell, ($) :: (a -> b) -> a -> b), the
  difference being that it stores the result in a global, hidden Map
  from function pointers and arguments to results. The user can then
  selectively memoize certain functions (or even just certain
  applications of the function), and not others -- the wise choice
  would be to not memoize cheap functions, but do memoize computation
  heavy functions. This is perfectly legal if the language is
  completely pure, as there can be no side-effects that are not
  repeated properly yada yada.

  An alternative could be that the user can mark a function definition
  as memoized, and then it's always memoized, not just certain
  applications. Also, there could then be a unique Map for each such
  function.
* INACTIVE Async I/O
  Zig seems to have a smart solution that doesn't require a separate
  `async` version of the standard library, unlike Rust with
  `async-std`.

  https://ziglang.org/download/0.6.0/release-notes.html#Async-IO

  Also look at how Haskell does it. It's probably smart.

* INACTIVE Boxing to allow for dynamic linking
  Boxing vs monomorphization. Boxing results in smaller binary and
  dynamically-linkable interface, but results in slower code (but not
  necessarily always, and maybe not by much!).

  Read /Tristan Hume - A Tour of Metaprogramming Models for Generics/
  for an overview of how different languages implement
  generics. [[https://thume.ca/2019/07/14/a-tour-of-metaprogramming-models-for-generics/][online]], [[file:~/Syncthing/books/papers/Tristan Hume - A Tour of Metaprogramming Models for Generics.html][locally]].

  When compiling a library, especially a dynamically linked one, how
  do we allow the export of polymorphic functions? We can't really use
  monomorphization, as we can't predict which types there should be
  instantiations for. Boxing would solve this problem and result in a
  smaller binary, but the code would most likely be slower, and the
  FFI would become more complicated.

  Maybe monomorphize all package-internal code, and require boxing for
  all public-facing polymorphic functions? Could require some keyword
  or special form, like `boxed`, to make it clear when the FFI will be
  affected.

  <2021-06-21 m√•n>: Try implementing polymorphism w boxing (& dict
  passing). Mono may really not be all that great, and it's really not
  that elegant. Big code size, slow compile times, no HRT, etc. Look
  at my own old post.

  https://www.reddit.com/r/ProgrammingLanguages/comments/npn3cd/what_are_some_anti_features_in_a_language/

  "With that said, I agree that eager monomorphization is an error, in my book.

   In a sense, monomorphization is exactly like inlining
   (copy/pasting). It feels strange that compilers would have complex
   heuristics to determine when to inline, when not to, and even in
   recent releases when to outline and yet... they just monomorphize
   everything template/generic without pause."

  Maybe box by default, and box all external functions, but like
  inlining, do monomorphization of appropriate function instantiaitons
  heuristically.

  From Tristan's text, on Haskell's dictionary passing:

  "Another way of implementing dynamic interfaces than associating
   vtables with objects is to pass a table of the required function
   pointers along to generic functions that need them. This approach
   is in a way similar to constructing Go-style interface objects at
   the call site, just that the table is passed as a hidden argument
   instead of packaged into a bundle as one of the existing arguments.

   This approach is used by Haskell type classes although GHC has the
   ability to do a kind of monomorphization as an optimization through
   inlining and specialization."

  See [[https://www.youtube.com/watch?v=ctS8FzqcRug][Switf's approach with the Value Witness Table]]. Basically,
  instead of passing generic types as completely opaque boxes, pass
  them as more of a sort of trait object, with some bundles functions
  for allocating and copying the type on the stack etc. Otherwise we
  have to store everything on the heap, even primitive types?

  Above paragraph is slightly misleading. Tristan explains witness
  tables well:

  "Swift makes the interesting realization that by using dictionary
   passing and also putting the size of types and how to move, copy
   and free them into the tables, they can provide all the information
   required to work with any type in a uniform way without boxing
   them. This way Swift can implement generics without
   monomorphization and without allocating everything into a uniform
   representation!  They still pay the cost of all the dynamic lookups
   that all boxing-family implementations pay, but they save on the
   allocation, memory and cache-incoherency costs. The Swift compiler
   also has the ability to specialize (monomorphize) and inline
   generics within a module and across modules with functions
   annotated @inlinable to avoid these costs if it wants to,
   presumably using heuristics about how much it would bloat the code.

   This functionality also explains how Swift can implement ABI
   stability in a way that allows adding and rearranging fields in
   structs, although they provide a @frozen attribute to opt out of
   dynamic lookups for performance reasons."

  This sounds really good! Single definition generation without
  expensive boxing! Monomorphization as an optimization!

  Value Witness Table in Swift seems to contain:
  
  - Size
  - Alignment
  - Copy constructor
  - Move constructor
  - Destructor

  If this was rust, .clone() would be an explicit call and a move
  wouldn't call any constructor or destructor, so the only things
  contained would be:

  - Size
  - Alignment
  - Destructor (Drop)

  We don't even have Drop yet, so the WVT only has to contain the
  type's size and alignment. Not much of a table heh...

  We'll have to do some kind of dictionary passing for the classes
  Cast, Num, Bitwise, and Ord I think.

  So for a polymorphic function, generate a single function that takes
  a reference to the value, a VWT (size, alignment), and dictionaries
  for any class constraints. In the generated code, use the VWT to get
  the size for when we need to allocate memory for the type, or
  memcpy. I'm thinking we won't need to though, right? Since it's
  already on the stack since it's behind a reference, we don't need
  the size for ~alloca~, and we only do store/load after a gep when
  indexing into the type, right? And that will only be done in
  monomorphic functions I believe.

  We must have what Swift calls "Metadata Patterns" as well. Say we
  have ~(define: (twice a) (Fun a [a . a]) (car (id [a . a])))~. We
  only pass the VWT of ~a~ to ~twice~, but we must also pass the VWT
  of ~(Pair a a)~ to ~id~, as well as the offset of the second element
  of the pair to ~car~. The second VWT and the rest of the metadata
  about the datatype must be constructed at runtime. So for every
  parametric datatype, we must generate a function that takes a VWT
  for each datatype parameter, and returns a /type metadata/
  value. The type metadata, beyond the VWT of the datatype, must also
  contain the offsets of each struct member.

  Metadata pattern example in Swift:

  #+BEGIN_EXAMPLE
  metadata pattern for Pair<T>   
  - first: T
  - second: T
  - value witness table

  metadata for Pair<Bool>
  - T: Bool
  - first: offset 0
  - second: offset 1
  - value witness table

  metadata for Pair<Int>
  - T: Int
  - first: offset 0
  - second: offset 4
  - value witness table
  #+END_EXAMPLE

  Generic member access in Swift:

  - Example:
    #+BEGIN_SRC swift
    func getSecond<T>(_ pair: Pair<T>) -> T {
        return pair.second
    }
    #+END_SRC
    
  - Implementation:
    #+BEGIN_SRC c
    void getSecond(opaque *result, opaque *pair, type *T) {
        type *PairOfT = get_generic_metadata(&Pair_pattern, T);
        const opaque *second =
            (pair + PairOfT->fields[1]);
        T->vwt->copy_init(result, second, T);
        PairOfT->vwt->destroy(pair, PairOfT);
    }
    #+END_SRC

  More things to consider when HOF:s are involved! https://youtu.be/ctS8FzqcRug?t=776

  Consider the case of a HOF accepting a monomorphic function. Something like:

  #+BEGIN_SRC carth
  (define: (apply f a)
      (forall (a) (Fun (Fun a a)
                       a
                       a))
    (f a))
  #+END_SRC

  Apply is a higher order function, and the type of the parameter ~f~
  is polymorphic (not higher ranked though). Therefore, in the lowered
  ~apply~, the lowered type of ~f~ will be something like
 
      void (*)(opaque *ret, opaque *arg, void *ctxt)
      
  What if we now have a simple, monomorphic function like ~neg~, of
  higher type ~(Fun Int Int)~. In the high domain, ~(Fun Int Int)~ is
  compatible with ~(Fun a a)~, but in the low domain,
  
      Int (*)(Int arg, void *ctxt)
      
  is not compatible with
  
      void (*)(opaque *ret, opaque *arg, void *ctxt)

  We thus need to generate an abstracting wrapper around concrete
  functions when passing them to a function that takes a non-concrete
  function as argument.

  Swift uses the terminology "Abstraction Patterns". "One formal type,
  many lowered representations". "Introduce thunks to translate
  between representations". To pass a concrete function as an abstract
  argument, they use what they call a "re-abstraction thunk". "We need
  to re-abstract the closure value, to match the abstraciton pattern
  of the function parameter. We do this using a thunk".

  The method itself is very obvious.

  #+BEGIN_SRC c
  Int closure(Int a) {
      return a + 1;
  }

  void thunk(Int *ret, Int *arg, void *thunk_ctxt) {
      Int (*fn_invoke)(Int, void*) = thunk_ctxt->...;
      void *fn_context = thunk_ctxt->...;
      ,*ret = fn_invoke(*arg, fn_context);
  }
  void *thunk_ctxt =allocate(..., closure, NULL);

  apply(..., thunk, thunk_ctxt, ...);
  #+END_SRC

* NEXT Add separate pass before Codegen to compile SrcPos:s
  I think it could be done purely and independently from rest of codegen. Would be more clean.
* NEXT Refactor & document Codegen & Gen
  It's getting big, complex, and unwieldy. Probably buggy as
  well. There's also a distinct lack of documentation. Always takes a
  sec for me to remember what some badly named function actually does.
* INACTIVE Use GADTs in Infer
* NEXT Have a look at LLVM.IRBuilder
  Might simplify my Codegen

  https://hackage.haskell.org/package/llvm-hs-pure-9.0.0/docs/LLVM-IRBuilder-Module.html#v:function

* INACTIVE Add basic repl
  Add a basic repl based on the JIT. Something very similar to
  http://www.stephendiehl.com/llvm/.

  Could maybe be the starting point for an on-demand architechture?
  Would probably require some memoization mechanism so that we don't
  unnecessarily check, monomorphise, and compile stuff we don't need
  to.
* NEXT Un-generalize module Selections
  Since we now use JIT instead of interpreter, only Codegen uses
  Selections, and we could make it simpler by inlining it.
* NEXT Type aliases
  Like ~type String = [Char]~ in Haskell.
* INACTIVE Query-based / on-demand compilation
  More or less a prerequisite to compile-time evaluation. Also enables
  good incremental compilation, and better IDE/LSP support.

  https://ollef.github.io/blog/posts/query-based-compilers.html
* INACTIVE Compile-time evaluation
  Could be used at different steps of compilation, for different purposes.

  - Procedural macros :: Can do more advanced generation.
  - Derive :: Using a similar mechanism to procedural macros, generate
    typeclass instances.
  - Conditional compilation :: If we for example allow comptime
    expressions evaluating to syntax at top level, we could use a
    mechanic similar to procedural macros for conditional
    compilation. Just have an if-expression on some compiler-defined
    global variable specifying e.g. what the platform is.
  - Dependent types :: Instead of having function and type-function
    definitions exist in separate spaces, like in Haskell, we could
    use normal functions. Could also use normal values, instead of
    having to redefine them at the type level (like having to define
    peano numbers and use datakinds in haskell).
  - Optimization :: Compute stuff att compiletime that can be computed
    at compiletime. Could probably use a mechanism similar to the
    dependent types to evaluate glob vars at compile time.

  Look at how zig, agda, and rust does it.

  Zig doesn't have macros -- their comptime only happens somewhere
  around the typechecking step. I think their comptime is evaluated by
  interpreting some mid-level IR. https://www.youtube.com/watch?v=8MbREuiLQrM

  Rust has constfn. Interpreting Miri.

  Agda idk.
  
  Query-based / on-demand compilation would make things *much*
  simpler, I'm fairly sure. Maybe even a prerequisite.

  proc-macros + parsing + mutual recursion seems like it might be a
  little tricky to solve. What if a proc-macro calls another
  proc-macro defined later in the file? Need to parse everything, so
  we can parse everything. Chicken and egg problem. Using Haskell
  laziness and ~fix~ might work. But the proc-macros don't just need
  to be parsed, but also typechecked and interpreted... Seems like
  tons of monadic complexity might surface.

  Do we do something like the typechecker, finding references and
  constructing a topological order of recursion groups ahead of time?
  Maybe use some kind of continuation-mechanism to exit parsing as
  soon as a proc-macro application is encountered, allowing resumption
  as soon as it has been defined?

  What about this: (direct or indirect) references to self must be at
  the "same level", i.e. you can't use self to generate the syntax of
  self, but you can call self as a normal (mutually) recursive
  function.

  So basically, if when doing query based compilation (which is depth
  first), and we reach a parsetime/macro application of self while
  still parsing self (i.e. it's in a stack of symbols of currently
  being parsed defs or smth), we return an error.

  Or maybe do like the typechecker and gather macro refs ahead of
  time. Like traverse the tree, and within all ~(parsetime ...)~ (or
  whatever) blocks, gather all referenced names. Do this for the while
  graph of referenced names recursively. In the end, we have a graph
  of all names necessary to parse the entry definition. Make a
  topological order. Compile them (to interpretable AST) in order. If
  there are any cyclical groups, compilation error.
* TODO Benchmark, profile, optimize
  Check out
  https://ollef.github.io/blog/posts/speeding-up-sixty.html. Great
  tips!
* INACTIVE Streamline learning the language
  Not that getting users is a primary concern, but if someone is
  indeed curious, I don't want them to be scared off by the process of
  getting started seeming complex.

  https://news.ycombinator.com/item?id=23347357
  https://www.hillelwayne.com/post/learning-a-language/
* NEXT Unify the different ASTs / IRs
  It's just kinda messy right now. Many files must be changed when
  touching just about any part of the AST representation. Also, takes
  up a lot of lines for not much apparent gain. Use some kind of
  attribute-tag to change the AST for different stages. Like:

  #+BEGIN_SRC haskell
  type Expr attr = Expr attr (Expr' attr)

  type ParsedExpr = Expr (Type, SrcPos)
  type CheckedExpr = Expr CheckedType
  #+END_SRC
* INACTIVE Optimize away zero-sized types before codegen
  It's bad that many operations on zero-sized types are currently
  actually compiled to, in practice, a ton of no-ops. I think it might
  be a good idea to add a dedicated optimization pass after
  monomorphization but before codegen that just gets rid of all
  zero-sized types and operations on them. For example, a type like
  ~(data Foo (Foo Bar Unit Baz))~ can be changed to ~(data Foo (Foo
  Bar Baz))~ without affecting the size of the generated struct
  etc. Also, a store of a ~{}~ into a ~{}*~ is really a no-op -- just
  noise in the generated LLVM. Being able to assume no zero-sized
  types in Gen/Codegen would also be really nice, I think.

  One issue: If you get rid of all ZSTs, what happens to a function
  with return-type Unit? What does it now return? One option could be
  to have add a special LLVM-Void type that just marks that the
  function should return void later. Another, more interesting option,
  would be to simply remove all functions and function-calls where the
  only remaining return type is a ZST, since, in purely functional
  programming, such a function can't do anything anyways. This would
  work, as long as *all* functions with side-effects are marked with
  IO & the RealWorld of IO is not a ZST & unsafePerformIO is known to
  the compiler and is (one of) the only (potentially) ZST-returning
  functions not optimized away, or unsafePerformIO returns something
  like ~(data (UnsafeIOResult a) (UnsafeIOResult a SizedMarker))~ to
  ensure the result is sized.

  Maybe do the flattening thing so there is only one zero sized type,
  but don't optimize away operations returning Unit completely. It
  would still be nice to be able to expect side effects and panics to
  happen. Also, RealWorld wouldn't have to have a size and actually
  impact performance.

* INACTIVE Builtin parsing of C header files
  I think Zig has this, and in Rust you can use the external tool
  ~bindgen~ to generate Rust declarations for C headers ahead of time.

  I just think it would be nice to not need to manually translate
  header files to use external libraries like OpenGL or SDL or
  whatever.
* INACTIVE Investigate alternative linkers
  Linking is one of the bottlenecks. However much caching etc I do in
  the parser & typechecker etc, the linker still has to do everything
  from scratch each time. I read somewhere that "gold" is a new GCC
  linker? Try using that maybe, unless it's already in use?

  https://news.ycombinator.com/item?id=24615916

  This is a new one: *mold*. It has as goal to be really fast. Seems promising!
  https://github.com/rui314/mold

* INACTIVE Produce .so:s for debug builds
  Linking is slow, so for debug builds we could try to split the
  output by module into separate .so:s. Then we'd only have to rebuild
  the .so of the affected module in incremental compilation.

  https://news.ycombinator.com/item?id=24615916

* INACTIVE Build Future into IO, or have both IO and AsyncIO?

* NEXT Some algorithms & data structures
  We need good collections & algs for sorting etc. if Carth is going
  to be of any use to anyone. Would also be a good way to add to the
  set of test-programs & find the worst pain points of current Carth.

  Many of these have implementations to look at and compare to on
  [[rosettacode.org]].

  This list is sort of off the top of my head, so some might not be
  good fits in a purely functional language. Look at some resource on
  persistend data structures as well.

  - Priority queue
  - Binary tree (2-3 tree better?)
  - B-tree (specifically 2-3 tree?)
  - Random number generator
  - bubble, insertion, selection sort
  - quicksort
* NEXT Don't actually define stuff like Str in the compiler
  Just assume they're defined by the user. Would mean less stuff in
  the compiler, and more in carth source. Both positives and
  negatives. I feel it would be nice as a user to be able to inspect
  the .carth source of the stdlib and actually see all the types and
  stuff though.
* INACTIVE Union types
  Like Typescript (I think, I'm not all that familiar with it). Could
  be nice for error handling, for example. That's one of the problems
  in Rust -- you have to use all these fancy crates or write a bunch
  of boilerplate just to allow a function to return two different
  types of errors.

  Java, where exceptions can be combined as a union, essentially:
  #+BEGIN_SRC java
  public Foo foo() throws SomeException, OtherException {
      bar(); // throws SomeException
      baz(); // throws OtherException
  }
  #+END_SRC

  and Rust, where you have to combine the different types somehow:
  #+BEGIN_SRC rust
  fn foo() -> Result<Foo, MyErr> {
      bar().map_err(MySomeErr)?;
      baz().map_err(MyOtherErr)?;
  }

  enum MyErr {
      MySomeErr(SomeErr),
      MyOtherErr(OtherErr)
  }
  #+END_SRC
* INACTIVE Hygienic macros
* INACTIVE Destructors
  System to register a function as a destructor for a value, which can
  be used to destroy / close resources when the value is no longer
  used and garbage collection happens. It's not optimal that resources
  may stay open for quite a while after last usage, but it's better
  than *never* being closed.

  Example use case: We don't want to have to use linear types to
  manually destroy Lazy values when we're done with them, but we still
  need to make sure that their mutexes are destroyed at some point.

  https://www.hboehm.info/gc/finalization.html
* NEXT "Use ptrtoint/inttoptr sparingly, prefer GEPs"
  https://llvm.org/docs/Frontend/PerformanceTips.html#other-things-to-consider

  I don't think I use ptrtoint/inttoptr much or at all in the compiler
  itself, but the ~ptr/+~ function in the stdlib transmutes to int for
  addition. Should add a builtin virtual function that uses gep to
  offset pointer.
* Pattern matching
** INACTIVE Var pattern syntax, comparison
  What if we did

  #+BEGIN_SRC carth
  (define (foo x pair)
    (match pair
      (case [x (let y)] (Some y))
      (case [_ _] None)))
  #+END_SRC

  instead of

  #+BEGIN_SRC carth
  (define (foo x pair)
    (match pair
      (case [x' y] (if (= x x')
                       (Some y)
                     None))))
  #+END_SRC
** INACTIVE Or-patterns
   Like in Rust. Very convenient.

   #+BEGIN_SRC rust
   match foo {
       (1, x) | (5, x) => x * 2,
       (_, y) => y,
   }
   #+END_SRC
** INACTIVE Active Patterns
   Like F# has. Something to
   consider. https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/active-patterns

   Could enable us to use pattern matching more?
   
* TODO Move from LLVM to alternative backend
  LLVM is kind of not great in some ways. It's often not trivial to
  debug errors stemming from displeasing LLVM. It updates frequently,
  but the Haskell bindings lag behind, so I have to use an older
  version or start maintainin llvm-hs myself. The project is
  *massive*, and most of the stuff I don't need. Sure, it's nice being
  able to target practically any backend, but I don't *actually* care
  about most of them. And there exists *so many* optimization passes,
  but most of them actually improve the performance of the binary very
  little, while bumping the compiletime a not insignificant bit.

  I want to use something simpler.

  To make the transition smooth, and to allow for easier debugging of
  codegen in the future, I think it would be a good idea to add an
  interpreter, like the one we had before, but now supporting FFI
  calls so that std-rs can be used as well. Really, the amount of code
  would not be huge, and it would be incredibly nice to have something
  to compare to when debugging low-level stuff. Also, I want to get
  rid of LLVM right away, but I'm not sure about what to replace it
  with just yet, so an interpreter is needed in the meantime.

** TODO Add low-level intermediate representation in Carth
   Would require less work to change backend or add multiple ones of I
   just have to translate from a low-level IR to the backend code,
   instead of all the way from an AST. Might also be good for the
   interpreter to run at a lower lever, but not sure.

   *UPDATE*: I'm warming up to focusing on this rather than the
   interpreter.

   Features the LIR should have (or maybe lack, rather):
   - Switches with sub-value extraction instead of pattern match.
   - No closures, but their representation in the form of function +
     environment instead.
   - Tail call optimized. (Replace tail-recursion &
     sibling-tail-recursion with loops or smth).
   - Beta reduction.
   - Detect fully saturated calls & have special ways of directly
     calling builtin virtuals, externs, and normal functions
     saturatedly.

   Thinking about alloca:s (stack allocations) in generated loops for
   e.g. tail call optimization. Is it fine to simply generate all the
   alloca:s as we do in the LLVM codegen, but maybe instead of placing
   the statements at the point of use, output them with a Writer monad
   and place them at the function entry. As long as the register names
   used are good, it should work out fine right? Similar to how we
   currently generate strings.

   Thinking about to what level we should lower the IR. Remain at
   nested expressions, or move on to blocks and goto:s? Blocks with
   parameters vs. Phi-nodes? If remain with if-expressions,
   translation to C would be much cleaner, but how do we create the
   loop for tail-recursion? If we go to block level, might be easier
   to generate MIR, LLVM, or even ASM, but what if we want to generate
   for some slightly higher level target like C?

   Mutual tail recursion and/or sibling calls seem more difficult to
   optimize, so maybe just guarantee optimization of tail-recursive
   calls for all backends & platforms, but rely on the backend for
   general sibling call optimization when supported. LLVM can do
   sibling calls, for example.

   Thinking about non-recursive tail calls. What is it that makes them
   difficult to optimize exactly I wonder. If we want to support stuff
   like continuation-passing style, general TCO would be quite
   necessary. Wiki sort of explains it: "However, for language
   implementations which store function arguments and local variables
   on a call stack (which is the default implementation for many
   languages, at least on systems with a hardware stack, such as the
   x86), implementing generalized tail call optimization (including
   mutual tail recursion) presents an issue: if the size of the
   callee's activation record is different from that of the caller,
   then additional cleanup or resizing of the stack frame may be
   required. For these cases, optimizing tail recursion remains
   trivial, but general tail call optimization may be harder to
   implement efficiently.". "As a result, functional languages such as
   Scala that target the JVM can efficiently implement direct tail
   recursion, but not mutual tail recursion.". If Scala can't do it,
   maybe it's fine if we can't either?

   http://web.eecs.umich.edu/~mahlke/courses/483f06/lectures/483L17.pdf

   I think I'll start with a very simplified version of Monomorphic,
   and possibly change it or add an additional even lower step
   afterwards.

   Detect tail recursive functions in lowering & mark the tail
   recursive calls. Should then be able to generate an efficient loop
   in LLVM / whatever, and should be able to not generate anything
   unnecessary.

   #+BEGIN_EXAMPLE
   f x y =
     if foo x y
     then f (x - 1, y)
     else g x y
   #+END_EXAMPLE

   becomes

   #+BEGIN_EXAMPLE
   @recursive=yes
   f x y =
     if foo x y
     then @recurse (x - 1, y)
     else g x y
   #+END_EXAMPLE

   If function is marked as recursive, the codegen knows to stack
   allocate the parameters so they can be modified for each iteration
   (could consider block-params / phi-nodes as alt., but this solution
   seems relatively simple). If special instruction to recurse is
   encountered, just set the parameter stack variables and jump to the
   entry label kept in Reader.

   https://github.com/bytecodealliance/wasmtime/blob/main/cranelift/docs/ir.md
   https://www.cs.tufts.edu/~nr/c--/extern/man2.pdf
   https://llvm.org/pubs/2009-10-TereiThesis.pdf
   https://llvm.org/devmtg/2015-10/slides/GroffLattner-SILHighLevelIR.pdf

*** INACTIVE Separate backend from Low and beyond into its own library?
    Maybe a separate package in the same repo?

    Not sure about this one, but maybe. The idea popped into my head
    that I could test Low out before implementing Lower if a write a
    simple C frontend or something for it, and just try compiling C
    code. But it would be kind of ugly for the carth package to
    contain a C frontend -- except it would not! it could be wicked
    useful for generating bindings for C libraries!! but I digress --
    so why not separate the backend into its own package, and the C
    frontend wouldn't really have to be "a part of" carth.

    Something to consider for sure.
** DONE Step 1: Re-add interpreter for pure Carth code
   Fairly self explanatory. Just operate on whatever is returned by
   the Optimize pass. Make sure to add / translate as many test-cases
   as possible to work without ~extern~ declarations, so that I can
   ensure as few correctness regressions as possible.

** INACTIVE Step 2: Support ~extern~ in interpreter
   This may not be trivial, but I think it won't be too hard. Can get
   some stuff from the codegen.

   Use [[https://hackage.haskell.org/package/libffi][libffi]] for dynamic FFI calls with runtime type info.

   How to convert data from Haskell to C? Functions for primitive
   types in libffi. For complex datatypes, I'm sure there's libraries
   for converting to bytes directly.

   Use sizeof and alignmentof from codegen module.

   *UPDATE*: Actually, this got complicated. How to handle GC roots,
   Haskell GC vs. Boehm GC. Allowing arbitrary extern calls, including
   those that might unsafely mutate memory. When we add our own GC
   with user-defineable destructor functions, how can we pass the
   user-defined function via FFI if it's a Haskell function basically?
   It all just gets really messy. Might not be much point in trying to
   do this after all... Focusing on adding our own LIR and using MIR
   for JIT/compilation seems like a better route at this point.

** NEXT Step 3: Remove LLVM support
   yeah

** INACTIVE Step 4: Add new native codegen backend
  Investigate QBE, Cranelift, GNU Lightning, libgccjit, GCC, MIR.

  #+BEGIN_QUOTE Candidates
  - C :: I.e., spit out C source and call out to ~cc~. Very portable
    (every platform has a C compiler). Not very elegant. Does not
    natively support tail call elimination, so would have to do that
    myself (true for pretty much everything except llvm though). Used
    by respectable languages like Nim and Haskell (sort of).
  - C-- :: Similar to C, but even more "portable assembly
    language". Created by SPJ and friend, specifically for being
    generated by compilers. Fork called Cmm used by GHC.
  - LLVM :: Approx 5 million LOC. Many targets, OK usability, but
    breaking changes sometimes and big and scary.
  - GCC :: Even bigger than LLVM. Also many targets. Not very good
    usability. Probably quite stable. GPL.
  - libgccjit :: Despite the name, also AOT. Basically an easier to
    use frontend for GCC with additional functionality to leverage GCC
    for JITting. Most points of GCC apply, but easier to use, and JIT
    included.
  - GNU Lightning :: JIT (only). Used by some schemes. Disjoint from
    GCC.
  - Cranelift :: Small-ish atm, but not sure it has any goals to stay
    that way. Seems more like an effort to replace LLVM, including
    much of its "bloat". Written in Rust. Maybe not all that
    standalone? Seems to be meant to be called from Rust. Performance
    of generated code seems bad atm, but should be improved.
  - QBE :: Small! 10k LOC. Goals to be 70% as fast as
    GCC/LLVM. Generates ASM instead of machine code for some
    reason. Seems like it hasn't seen much update this last
    year. However, one [[https://github.com/michaelforney/qbe][Michael Forney is actively maintaining a fork]],
    for his own language I think, so that might be interesting.
  - [[https://github.com/vnmakarov/mir][MIR]] :: This one looks the most interesting! Similarly to QBE, very
    small at 15k LOC and 70% the performance of GCC. Primarily a
    JIT(?), but seems to be able to to AOT as well. Has a 4 backends
    atm, including AMD64 and Aarch64, and it seems relatively easy to
    add a new one. I've found 2 languages that make use of MIR to
    study: [[https://github.com/grame-cncm/faust][Faust]] and [[https://github.com/dibyendumajumdar/ravi][Ravi]].
  #+END_QUOTE

  In the end, I most like the look of MIR. It seems to make good
  tradeoffs.

  Compiling to C comes at second place. Incredibly portable, and .c
  files would be a lot more readable than .ll files. Would lose the
  GDB source-line from DWARF stuff though, but that shit kinda sucked
  anyways. Function names would work as well, if not better than in
  LLVM, since the names would be kept in the C, and C compilers
  probable output much better dwarf than I ever could.

  Maybe I'll do both? If I just a low-level IR that's just above the
  level of the union of C and MIR it ought to be quite simple to
  translate from that to whatever backendest backend.

  Ravi, a language using MIR: https://github.com/dibyendumajumdar/ravi

** References
   - [[https://gist.github.com/zeux/3ce4fcc3a43072b4315abde95319ecb6][How does clang 2.7 hold up in 2021?]]
* NEXT Try our an alternative prelude, like relude
* TODO `tail` keyword to ensure tail call or compiler error
  Sometimes you want to be sure that tail calls are optimized. To be
  able to assert this at compile time, so as to not accidentally
  create a stack consuming function when it really matters, add a
  `tail` keyword.

  TCO should already performed as an optimization, but with `tail`,
  you can ensure that you get a compiler error if the call is not
  actually a tail call, if you've done something wrong or
  something. Sort of like Rust is considering the `become` keyword to
  work?
* TODO Cleaner method of producing useful stack traces
  I don't really like how we do source positions now, annotating
  everything and generating bad DWARF in the LLVM
  backend. Line-by-line stepping won't work well regardless of how we
  do, due to the expression-oriented nature of the language. Stack
  traces is the most interesting thing we want for debugging by far,
  and that might be achievable with some more general and cleaner
  method. Some kind of shadow stack, for example. Such a method might
  work well for other potential backends as well, like a C backend.

* INACTIVE Add kind of ~apply~ function that takes tuple
  #+BEGIN_SRC carth
  (define (foo a b c)
    (+ a (* b c)))

  (assert-eq (foo 1 2 3) (apply foo [1 2 3]))
  (assert-eq (foo 1 2) (apply foo [1 2]))
  #+END_SRC

  In general, ~(apply f [x1 ... xn])~ becomes ~(f x1 ... xn)~.

  I think it could be a function, via a type class instance that
  recurses on the pairs of a tuple.

  One usage that could be nice in particular is when you want to apply
  a function with "default" arguments. You could then do ~(apply f
  default)~ instead of anything more complex.

  Then again, you can do something arguably more convenient with
  typeclasses and deriving in haskell. Create a record for the
  specific argument set, derive Default, and call it like ~f (default
  {foo = 3})~.
  
* INACTIVE SoA record attribute
  https://blog.royalsloth.eu/posts/the-compiler-will-optimize-that-away/

  Convenient syntax for using SoA/AoS could be nice for lowe level
  stuff, or we might consider it too seldom an issue for a somewhat
  high-level languge like Carth.
* INACTIVE Recursion schemes
  Recursion schemes are functions that capture patterns of recursion,
  like fold and unfold. These 2 are simple to implement. Other
  schemes, less commonly used yet frequently applicable, like cata,
  could be implemented as well, but might require some built in
  support or smart "deriving".

  Look at https://hackage.haskell.org/package/recursion-schemes-5.2.2.1

  Maybe deriving functor and/or foldable could include this base
  functor thingy?

* INACTIVE Borrow checking
  Don't think I'll implement anything like this. There's Carp or Rust
  or whatever if you prefer that. I kind of want a nice GC actually.

  But anywho, in case we ever want to add borrow checking, I'll
  collect some useful notes here.

  Check out Polonius, the new borrow checker in Rust. https://youtu.be/H54VDCuT0J0

** TODO Dead code elimination of externs & wrappers
   We already do dead code elim almost by mistake in Monomorphize, but
   we still generate declarations and wrappers for all
   ~extern~:s. Getting rid of them would be nice.
   
* INACTIVE GPU targetable
  Either in Carth directly, or via a DSL or something. Some method of
  doing flattening and parallelisation like Futhark? Compile to OpenGL
  & Vulkan maybe.

* NEXT Write c compiler i carth
  Look at tutorials. There are many minimal c compilers. tinycc(?) is one, IIRC.

  At first, just a fun exercise. Seeing how well Carth fares at such a
  task. Discovering new bugs & limitations of the compiler. Coming up
  with new features.

  In the future, may be integrated in a self-hosted Carth compiler for
  C header parsing support, or even full-on C source library
  support. Kind of like Zig.

* NEXT Sugar for lambdas
  Look at [[https://clojure.org/guides/learn/functions#_anonymous_function_syntax][Clojure's reader shorthand for anonymous functions]].

  It's basically De Brujin notation. So ~(fn [a b] (* 5 (+ a b)))~ can
  also be written ~#(* 5 (+ %1 %2))~. That's convenient! If one
  instead does good point-free compositioning, like ~(<oo (* 5) +)~,
  the sugar is "unneccesary", but it really is quite concise and
  readable. Might be nice to have.

* NEXT Look at these languages
  For inspiration, learn from their mistakes, etc.
  
  Also add related work to readme, after looking closer at it, if applicable.

  - Hackett
  - Liskell
  - Axellang
  - Kalyn
    https://intuitiveexplanations.com/tech/kalyn#preliminary-technical-design-decisions
  - Unison
    https://github.com/unisonweb/unison
* NEXT Refactor type checker
  keywords: type checking, inferenc, inferrer

  I'm not completely happy with the typechecking. 4 module files
  (Check, Checked, Infer, Inferred) totalling over 900 SLOC. Also,
  ~solve~ is not just run once at the outermost level, visiting each
  constraint at most once. Because of nested ~let~ with polymorphism,
  we currently run ~solve~ nestedly, and in total, each constraint is
  likely visited more than once. This is ugly.

  See:
  - https://gilmi.me/blog/post/2021/04/06/giml-type-inference

* INACTIVE Nonstrict parameters
  Similar to how you can mark parameters as strict and force
  evaluation in Haskell, we could benefit from having params marked as
  nonstrict similarly.

  Then we can write functions that perform some sort of
  short-circuiting logic, like ~or~, ~parse/or~, ~maybe/or~, ~maybe~,
  etc, without having to resort to macros or explicit wrappings of
  ~(fun (Unit) ...)~s.

  It could look something like this
  
  #+BEGIN_SRC carth
  (define (or p #nonstrict q)
    (if p True q))

  (or (cheap-computation ...)
      (expensive-computation ...))
  #+END_SRC

  Also consider the nested case

  #+BEGIN_SRC carth
  (define (foo a b #nonstrict computation)
    (if (bar a)
        (baz b computation)
      3))

  (define (baz b #nonstrict computation)
    (or b (f computation)))
  #+END_SRC

* INACTIVE Better unicode support
  Possibly using Rust's builtin stuff. Also possibly use some Zig library?

  Otherwise, this Suckless library seems quite nice: https://libs.suckless.org/libgrapheme/

  Very small! That's always a plus :)

* INACTIVE Dynamic dispatch
  Like Box<dyn TRAIT> in Rust. Might be useful in places. Should not
  be that hard to implement -- just heap allocate a vtable, and
  populate it with all of the class functions. Might need to add
  wrappers so that the functions always accept the type by reference?
  Or all args by reference? Unless we modify the compiler to *always*
  pass args by reference. In Rust, I suppose they defer the problem by
  only allowing one to call ~&self~ and optionally ~&mut self~ methods
  on a trait objects. Don't have to consider sizes if you can't even
  call ~self~ methods in the first place.

  Must consider how this interacts with monomorphization vs. boxing
  vs. value witness tables for static dispatch.b
* TODO Don't curry by default
  I did currying by default mostly because it's the Haskell way of
  doing things, but this has, among other things, performance
  implications. And we're not Haskell, we're a Lisp.

  Maybe let's do function application a bit more like Clojure? We
  reduce the complexity of our call stacks and make profiling more
  clear and easy. We'd also improve performance by not having to heap
  allocate as many closure captures for the wrapping
  curry-closures. Also, variadic special forms and macros wouldn't
  cause confusion about whether it's a curried application or an extra
  argument. Maybe we could even have variadic functions in some
  way. ~(+ 1 2 3)~, ~(|> x f g h)~.

  In Clojure, one direct way of making a curried application is with
  ~partial~. 

  #+BEGIN_SRC carth
  ((cons 1) [2]) ;; error
  ((partial cons 1) [2]) ;; [1 2]
  #+END_SRC

  We probably couldn't implement it as a function, but we could have
  it as a special form. Or, I guess we could implement it by doing
  Rust-style
  implement-a-class-for-every-arity-version-up-to-a-big-number-like-11. Another
  possibility might be if function arguments were behind the scenes a
  tuple. Then we might implement it like

  #+BEGIN_SRC carth
  (define: (partial f a)
      (Fun [a . bs] c)
    (fun bs (f (cons a bs))))
  #+END_SRC
  
  This might have other negative implications, however.

  I suppose it would be nice if we could treat the function
  paramer-list as if it were a tuple, but under the scenes they
  actually don't, and the tuple stuff is just wrapped around it, and
  optimized away when calls are saturated. Then you could fmatch
  directly on the whole argument list, e.g.

  #+BEGIN_SRC carth
  (foldl (fmatch (case [(Left n) x]
                       (Left (inc n)))
                 (case [(Right xs) x]
                       (if (> 10 x)
                           (Right (list/cons x xs))
                         (Left 1))))
         ...)
  #+END_SRC

  --------------------------------------

  If we don't do currying, we could also better fit a nice
  arbitrary-position anonymous function syntax into Carth. Like
  Clojure's,
  https://clojure.org/guides/learn/functions#_anonymous_function_syntax.

  #+BEGIN_SRC clojure
  ;; Equivalent to: (fn [x] (+ 6 x))
  #(+ 6 %)

  ;; Equivalent to: (fn [x y] (+ x y))
  #(+ %1 %2)

  ;; Equivalent to: (fn [x y & zs] (println x y zs))
  #(println %1 %2 %&)
  #+END_SRC

  Or like Scala's (optionally typed) holes:

  #+BEGIN_EXAMPLE
  ;; In this example, _ is not a hole, but a partial application placeholder

  (+ 10 _) ;; Type of placeholder inferred to Int

  (define (pair x y) [x y])
  (pair 5 (: _ Nat)) ;; Type Nat ascribed to placeholder, to lock it
  #+END_EXAMPLE
* INACTIVE Have error messages quote section numbers for the spec
  when there is a spec.

  Would be nice, to have concrete documentation for what is ok and what is not.
* NEXT Dump everythiong to Graphviz
  Particularly the pre-LLVM ASTs. They're very hard to read as text,
  but would probably fit really well as a graph. This could be useful
  both for debugging the compiler, as well as to debug compiled
  programs.
